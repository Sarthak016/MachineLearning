{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V8hvmS8B8Uub"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarthak016/MachineLearning/blob/main/REGRESSION_ALL_BLUEPRINT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b><u> Project Title : Seoul Bike Sharing Demand Prediction </u></b>\n",
        "\n"
      ],
      "metadata": {
        "id": "7Vxy3OgU9Tgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Problem Description </b>\n",
        "\n",
        " Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "\n",
        "Our objective is to find a way to estimate the value prediction of bike count required at each hour for the stable supply of rental bikes. using the values in the other columns. If we can do so for the historical data, then we should able to estimate bike count required at each hour."
      ],
      "metadata": {
        "id": "sG6-L1a49ZcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "3g_njVc0Z009"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Data Description </b>\n",
        "\n",
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "\n",
        "### <b>Attribute Information: </b>\n",
        "\n",
        "*  Date : year-month-day\n",
        "*  Rented Bike count - Count of bikes rented at each hour\n",
        "*  Hour - Hour of he day\n",
        "*  Temperature-Temperature in Celsius\n",
        "*  Humidity - %\n",
        "*  Windspeed - m/s\n",
        "*  Visibility - 10m\n",
        "*  Dew point temperature - Celsius\n",
        "*  Solar radiation - MJ/m2\n",
        "*  Rainfall - mm\n",
        "*  Snowfall - cm\n",
        "*  Seasons - Winter, Spring, Summer, Autumn\n",
        "*  Holiday - Holiday/No holiday\n",
        "*  Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ],
      "metadata": {
        "id": "To4IuA_nxz6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "XdZ_fVCiZ_ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> All the Lifecycle In A Data Science Projects\n",
        "1. Data Analysis\n",
        "2. Feature Engineering\n",
        "3. Feature Selection\n",
        "4. Model Building\n",
        "5. Model Deployment"
      ],
      "metadata": {
        "id": "enOJlGRGaD_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "vgoJdoD4aIYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Libraries and Data** \n"
      ],
      "metadata": {
        "id": "ojBFx6ti9lbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from numpy import math\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext google.colab.data_table\n",
        "\n",
        "import pandas as pd\n",
        "pd.pandas.set_option('display.max_columns',None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn import neighbors\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "\n",
        "import lightgbm "
      ],
      "metadata": {
        "id": "RkGPoPxNahoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive to load data.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OBgxyGKuast4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the csv file\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/Capstone 2/REGRESSION/bike sharing demand prediction/SeoulBikeData.csv\",encoding= 'unicode_escape')"
      ],
      "metadata": {
        "id": "9OP3PQR9xiyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **First Look**"
      ],
      "metadata": {
        "id": "PktFwmS6a05W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fisrt 5 values.\n",
        "data.head()"
      ],
      "metadata": {
        "id": "K9_jXt1F-f91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Last 5 values.\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "1u_Ksg5wbPU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Let's check the duplicate entries"
      ],
      "metadata": {
        "id": "lV5HZ6kgbSa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicated entries.\n",
        "print(\"Duplicate entry in data:\",len(data[data.duplicated()])) "
      ],
      "metadata": {
        "id": "niFnx6Wk64jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Function for Dtype,Unique values and Null values\n",
        "def datainfo():\n",
        "    temp_ps = pd.DataFrame(index=data.columns)\n",
        "    temp_ps['DataType'] = data.dtypes\n",
        "    temp_ps[\"Non-null_Values\"] = data.count()\n",
        "    temp_ps['Unique_Values'] = data.nunique()\n",
        "    temp_ps['NaN_Values'] = data.isnull().sum()\n",
        "    temp_ps['NaN_Values_Percentage'] = (temp_ps['NaN_Values']/len(data))*100 \n",
        "    return temp_ps"
      ],
      "metadata": {
        "id": "2acvprqDbX0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the data.\n",
        "print(\"Total Rows and Columns in DataFrame is :\",data.shape,\"\\n\") \n",
        "# Custom Function\n",
        "datainfo()"
      ],
      "metadata": {
        "id": "QR_6Rl_7baJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 8760 rows and 14 columns. Each row of the dataset contains information about weather conditions\n",
        "\n",
        ">Our objective is to find a way to estimate the value prediction of bike count required at each hour for the stable supply of rental bikes. using the values in the other columns. If we can do so for the historical data, then we should able to estimate bike count required at each hour.\n",
        "\n",
        "\n",
        "> Looks like \"Seasons\", \"Holiday\", \"Functioning Day\" are strings (possibly categories) and rest columns are numerical data. None of the columns contain any missing values, which saves us a fair bit of work!\n",
        "\n",
        "Here are some statistics for the numerical columns:"
      ],
      "metadata": {
        "id": "6jjFlM4JbdUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical info.\n",
        "data.describe().T"
      ],
      "metadata": {
        "id": "NAt8PEnzbgti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ranges of values in the numerical columns seem reasonable too, so we may not have to do much data cleaning or correction. The \"Wind speed\",\"Dew point temperature(°C)\", \"Solar Radiation\", \"Rainfall\" and \"Snowfall\" column seems to be significantly skewed however, as the median (50 percentile) is much lower than the maximum value."
      ],
      "metadata": {
        "id": "C0DMkjx5x0rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Exploratory Analysis and Visualization\n",
        "\n",
        "Let's explore the data by visualizing the distribution of values in some columns of the dataset, and the relationships between \"Rented Bike count\" and other columns."
      ],
      "metadata": {
        "id": "Agbs6-ljyGgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Missing values**"
      ],
      "metadata": {
        "id": "n0UAQgDZyQRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 -step make the list of features which has missing values\n",
        "feature_with_na=[feature for feature in data.columns if data[feature].isnull().sum()>1]\n",
        "\n",
        "# 2- step print the feature name and the percentage of missing values\n",
        "for feature in feature_with_na:\n",
        "  print(feature, np.round(data[feature].isnull().mean(), 4)*100,  \" % missing values\")"
      ],
      "metadata": {
        "id": "14IzWrXlFLKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets drop columns which have nan value above 40%\n",
        "perc=40.0\n",
        "min_count=int(((100-perc)/100)*data.shape[0] + 1)\n",
        "data=data.dropna(axis=1,thresh=min_count)"
      ],
      "metadata": {
        "id": "qTo0P04LJWIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We'll use Dataprep library for automated visualization."
      ],
      "metadata": {
        "id": "0eR1bcksb3zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U dataprep"
      ],
      "metadata": {
        "id": "XCaSXG0tzsTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using dataprep profiling to get a idea about dataset \n",
        "from dataprep.eda import create_report\n",
        "report=create_report(data)\n",
        "report"
      ],
      "metadata": {
        "id": "4y_fmdPsLwBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Numerical Data"
      ],
      "metadata": {
        "id": "V8hvmS8B8Uub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of numerical variables\n",
        "numerical_features=[col for col in data.columns if data[col].dtype!='O']\n",
        "# Separate dataframe for Numerical feature\n",
        "num_data=data[numerical_features]\n",
        "num_data.head()"
      ],
      "metadata": {
        "id": "v9v-tvZi8TAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Data"
      ],
      "metadata": {
        "id": "BIkxJE4Nc8SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of categorical variables\n",
        "categorical_features=[col for col in data.columns if data[col].dtype=='O']\n",
        "# Separate dataframe for Categorical feature\n",
        "cat_data=data[categorical_features]\n",
        "cat_data.head()"
      ],
      "metadata": {
        "id": "ygsMywUudHNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discrete Variables"
      ],
      "metadata": {
        "id": "JeHqGBYZ8eUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets analyse the discrete values by creating histograms to understand the distribution\n",
        "discrete_feature=[feature for feature in numerical_features if len(data[feature].unique())<25]\n",
        "print(\"Discrete Variables Count: {}\".format(len(discrete_feature)))\n",
        "\n",
        "for feature in discrete_feature:\n",
        "    dataset=data.copy()\n",
        "    fig, ax = plt.subplots(figsize=(12,6),facecolor=\"#363336\")\n",
        "    ax.patch.set_facecolor('#8C8C8C')\n",
        "    dataset.groupby(feature)['Rented Bike Count'].median().plot.bar(color='red')\n",
        "    ax.tick_params(axis='x', colors='#F5E9F5',labelsize=15) \n",
        "    ax.tick_params(axis='y', colors='#F5E9F5',labelsize=15)\n",
        "    ax.set_xlabel(feature, color='#F5E9F5', fontsize=20)\n",
        "    ax.set_ylabel(\"Rented Bike Count\",  color='#F5E9F5', fontsize=20)       "
      ],
      "metadata": {
        "id": "gbfdasm18iW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Continuous Variables"
      ],
      "metadata": {
        "id": "v5iiBYyN8sBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets analyse the continuous values by creating histograms to understand the distribution\n",
        "continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\n",
        "print(\"Continuous feature Count {}\".format(len(continuous_feature)))\n",
        "\n",
        "for feature in continuous_feature:\n",
        "    dataset=data.copy()\n",
        "    fig, ax = plt.subplots(figsize=(12,6),facecolor=\"#363336\")\n",
        "    ax.patch.set_facecolor('#8C8C8C')\n",
        "    sns.distplot(dataset[feature],color='r',kde_kws={'linewidth':3,'color':'#4B0751'});\n",
        "    ax.tick_params(axis='x', colors='#F5E9F5',labelsize=15) \n",
        "    ax.tick_params(axis='y', colors='#F5E9F5',labelsize=15)\n",
        "    ax.set_xlabel(feature, color='#F5E9F5', fontsize=20)\n",
        "    ax.set_ylabel(\"Count\",  color='#F5E9F5', fontsize=20)\n",
        "   "
      ],
      "metadata": {
        "id": "2PaH7IJ-8pu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Categorical Variables"
      ],
      "metadata": {
        "id": "LRMc_PbJ8x_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique number of categorical features\n",
        "for feature in categorical_features:\n",
        "    print('The feature is {} and number of categories are {}'.format(feature,len(data[feature].unique())))"
      ],
      "metadata": {
        "id": "OLCa9J4I9F_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find out the relationship between categorical variable and dependent feature Rented Bike Count\n",
        "for feature in categorical_features:\n",
        "    fig, ax = plt.subplots(figsize=(12,6),facecolor=\"#363336\")\n",
        "    ax.patch.set_facecolor('#8C8C8C')\n",
        "    dataset.groupby(feature)['Rented Bike Count'].median().plot.bar(color='red')\n",
        "    ax.tick_params(axis='x', colors='#F5E9F5',labelsize=15) \n",
        "    ax.tick_params(axis='y', colors='#F5E9F5',labelsize=15)\n",
        "    ax.set_xlabel(feature, color='#F5E9F5', fontsize=20)\n",
        "    ax.set_ylabel(\"Rented Bike Count\",  color='#F5E9F5', fontsize=20)\n",
        "  "
      ],
      "metadata": {
        "id": "q_yVJCG09JXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers"
      ],
      "metadata": {
        "id": "ppoplBOm9NS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in numerical_features:\n",
        "    dataset=data.copy()\n",
        "    if 0 in dataset[feature].unique():\n",
        "        pass\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(12,6),facecolor=\"#363336\")\n",
        "        ax.patch.set_facecolor('#8C8C8C')\n",
        "        sns.boxplot(data[feature],color='red')\n",
        "        ax.tick_params(axis='x', colors='#F5E9F5',labelsize=15) \n",
        "        ax.tick_params(axis='y', colors='#F5E9F5',labelsize=15)\n",
        "        ax.set_xlabel(feature, color='#F5E9F5', fontsize=20)\n",
        "          "
      ],
      "metadata": {
        "id": "H9jbH4Sd9Mga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Relation"
      ],
      "metadata": {
        "id": "p96MffCnd6gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating scatterplot to determine the co-relation\n",
        "for col in (numerical_features[1:]):\n",
        "  fig, ax = plt.subplots(figsize=(12,6),facecolor=\"#363336\")\n",
        "  ax.patch.set_facecolor('#8C8C8C')\n",
        "  \n",
        "  sns.scatterplot(data = data, x = col ,  y = 'Rented Bike Count' ,hue = 'Seasons',s=250,palette=[\"pink\",\"grey\",\"black\",\"red\"], ax =ax)  #... using Season as hue to see the distribution of count\n",
        "\n",
        "  z = np.polyfit(data[col], data['Rented Bike Count'], 1)  # creating best fit line\n",
        "  y_hat = np.poly1d(z)(data[col])\n",
        "  plt.plot(data[col], y_hat, \"b--\", lw=4)\n",
        "  \n",
        "  ax.tick_params(axis='x', colors='#F5E9F5',labelsize=15) \n",
        "  ax.tick_params(axis='y', colors='#F5E9F5',labelsize=15)\n",
        "  ax.set_xlabel(col, color='#F5E9F5', fontsize=20)\n",
        "  ax.set_ylabel(\"Rented Bike Count\",  color='#F5E9F5', fontsize=20)"
      ],
      "metadata": {
        "id": "QifAs8hRd5kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation"
      ],
      "metadata": {
        "id": "8ROgVt_m-Nga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(abs(data.corr()),annot=True,cmap='coolwarm',linewidth=1,linecolor='black')"
      ],
      "metadata": {
        "id": "SHt21HQn-K-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Prepare the Dataset for Training\n",
        "\n",
        "\n",
        "Before we can train the model, we need to prepare the dataset. Here are the steps we'll follow:\n",
        "\n",
        "1. Identify the input and target column(s) for training the model.\n",
        "2. Identify numeric and categorical input columns.\n",
        "3. [Impute](https://scikit-learn.org/stable/modules/impute.html) (fill) missing values in numeric columns\n",
        "4. [Scale](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a $(0,1)$ range.\n",
        "5. [Encode](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) categorical data into one-hot vectors.\n",
        "6. Split the dataset into training and validation sets.\n"
      ],
      "metadata": {
        "id": "0WwkH6uC9_EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify Inputs and Targets\n",
        "\n",
        "While the dataset contains `81` columns, not all of them are useful for modeling. Note the following:\n",
        "\n",
        "- The first column `Id` is a unique ID for each house and isn't useful for training the model.\n",
        "- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.\n",
        "- Data from all the other columns (except the first and the last column) can be used as inputs to the model.\n",
        " "
      ],
      "metadata": {
        "id": "s2qnXMj_h2z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 4**: Create a list `input_cols` of column names containing data that can be used as input to train the model, and identify the target column as the variable `target_col`."
      ],
      "metadata": {
        "id": "zvWhINVt0egw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the input columns (a list of column names)\n",
        "input_cols = list(data.columns)[1:-1]\n",
        "\n",
        "# Identify the name of the target column (a single string, not a list)\n",
        "target_col =list(data.columns)[-1]"
      ],
      "metadata": {
        "id": "7T7AeTcxhvzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice whatever code u execute, print and check it \n",
        "print(input_cols)"
      ],
      "metadata": {
        "id": "fc-DatjX04qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice whatever code u execute, print and check it \n",
        "print(target_col)"
      ],
      "metadata": {
        "id": "7HHkRWNm03dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure that the `Id` and `SalePrice` columns are not included in `input_cols`.\n",
        "\n",
        "Now that we've identified the input and target columns, we can separate input & target data."
      ],
      "metadata": {
        "id": "JsdUeloo0wJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate input & target data\n",
        "inputs_df = data[input_cols]\n",
        "targets = data[target_col]"
      ],
      "metadata": {
        "id": "TQJmCsQo095e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Identify Numeric and Categorical Data\n",
        "The next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column."
      ],
      "metadata": {
        "id": "EwazBRPDiNap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 5**: Crate two lists `numeric_cols` and `categorical_cols` containing names of numeric and categorical input columns within the dataframe respectively. Numeric columns have data types `int64` and `float64`, whereas categorical columns have the data type `object`.\n",
        ">\n",
        "> *Hint*: See this [StackOverflow question](https://stackoverflow.com/questions/25039626/how-do-i-find-numeric-columns-in-pandas). "
      ],
      "metadata": {
        "id": "d9ocavp91cdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# identifying Numerical and Categorical columns\n",
        "#numerical=medical.select_dtypes(include=np.number).columns.tolist()\n",
        "numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = inputs_df.select_dtypes(include=[object]).columns.tolist()"
      ],
      "metadata": {
        "id": "Vr6XCGsPiNAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Impute Numerical Data\n",
        "Some of the numeric columns in our dataset contain missing values (nan)"
      ],
      "metadata": {
        "id": "F5XbLVc2ic2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using isna() to calculate the null values in Numeric columns\n",
        "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\n",
        "missing_counts[missing_counts > 0]"
      ],
      "metadata": {
        "id": "l3FsYhw4iecs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning models can't work with missing data. The process of filling missing values is called [imputation](https://scikit-learn.org/stable/modules/impute.html).\n",
        "\n",
        "<img src=\"https://i.imgur.com/W7cfyOp.png\" width=\"480\">\n",
        "\n",
        "There are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`.\n"
      ],
      "metadata": {
        "id": "OfIDO-cv1uB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 6**: Impute (fill) missing values in the numeric columns of `inputs_df` using a `SimpleImputer`. "
      ],
      "metadata": {
        "id": "91YfP3zG18Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SimpleImputer from sklearn library\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 1. Create the imputer\n",
        "imputer = SimpleImputer(strategy = 'mean')\n",
        "\n",
        "# 2. Fit the imputer to the numeric colums\n",
        "imputer.fit(inputs_df[numeric_cols])\n",
        "\n",
        "# 3. Transform and replace the numeric columns\n",
        "inputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])"
      ],
      "metadata": {
        "id": "NUGBkrYH1p8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using isna()  to check the null values in Numeric columns\n",
        "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\n",
        "missing_counts[missing_counts > 0]"
      ],
      "metadata": {
        "id": "bEIEeYBH1_Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scale Numerical Values\n",
        "The numeric columns in our dataset have varying ranges."
      ],
      "metadata": {
        "id": "K0xV7nPui-VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using describe function to see statistics information and .loc to filter min and max from describe function\n",
        "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
      ],
      "metadata": {
        "id": "KkHP1jBNjCoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good practice is to [scale numeric features](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) to a small range of values e.g. $(0,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.\n"
      ],
      "metadata": {
        "id": "RfHBHiPI2JU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 7**: Scale numeric values to the $(0, 1)$ range using `MinMaxScaler` from `sklearn.preprocessing`."
      ],
      "metadata": {
        "id": "SEZvAGJt2NRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MinMaxScaler from sklearn library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the numeric columns\n",
        "scaler.fit(inputs_df[numeric_cols])\n",
        "\n",
        "# Transform and replace the numeric columns\n",
        "inputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])"
      ],
      "metadata": {
        "id": "xRcn3CHs2F8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After scaling, the ranges of all numeric columns should be (0, 1)."
      ],
      "metadata": {
        "id": "o-l3QJBm2ccj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check that scaling worked or not\n",
        "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
      ],
      "metadata": {
        "id": "UioP8mNB2gq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encode Categorical Columns\n",
        "Our dataset contains several categorical columns, each with a different number of categories."
      ],
      "metadata": {
        "id": "tl5Hio73jG0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing unique Categorical columns \n",
        "inputs_df[categorical_cols].nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "osf6IG2mjIgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.\n",
        "\n",
        "<img src=\"https://i.imgur.com/n8GuiOO.png\" width=\"640\">\n",
        "\n",
        "One hot encoding involves adding a new binary (0/1) column for each unique category of a categorical column."
      ],
      "metadata": {
        "id": "K-lHn31D2riG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 8**: Encode categorical columns in the dataset as one-hot vectors using `OneHotEncoder` from `sklearn.preprocessing`. Add a new binary (0/1) column for each category"
      ],
      "metadata": {
        "id": "crFnxAFD2zHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import OneHotEncoder from sklearn library\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 1. Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "\n",
        "# 2. Fit the encoder to the categorical colums\n",
        "encoder.fit(inputs_df[categorical_cols])\n",
        "\n",
        "# 3. Generate column names for each category\n",
        "encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
        "len(encoded_cols)"
      ],
      "metadata": {
        "id": "FZawE_Yx28g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Transform and add new one-hot category columns\n",
        "inputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])"
      ],
      "metadata": {
        "id": "UeiagrpW3BNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new one-hot category columns should now be added to `inputs_df`."
      ],
      "metadata": {
        "id": "2mnsgZ1o3F1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = inputs_df[numeric_cols + encoded_cols]\n",
        "target = targets"
      ],
      "metadata": {
        "id": "TReCpBoAQM1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "aNIjHf6QP7P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "# to visualise al the columns in the dataframe\n",
        "pd.pandas.set_option('display.max_columns', None)\n",
        "\n",
        "### Apply Feature Selection\n",
        "# first, I specify the Lasso Regression model, and I\n",
        "# select a suitable alpha (equivalent of penalty).\n",
        "# The bigger the alpha the less features that will be selected.\n",
        "\n",
        "# Then I use the selectFromModel object from sklearn, which\n",
        "# will select the features which coefficients are non-zero\n",
        "\n",
        "feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\n",
        "feature_sel_model.fit(input, target)\n"
      ],
      "metadata": {
        "id": "YI8SsLkRQCuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_sel_model.get_support()"
      ],
      "metadata": {
        "id": "Utyxq5XhQFw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's print the number of total and selected features\n",
        "# this is how we can make a list of the selected features\n",
        "selected_feat = input.columns[(feature_sel_model.get_support())]\n",
        "\n",
        "# let's print some stats\n",
        "print('total features: {}'.format((input.shape[1])))\n",
        "print('selected features: {}'.format(len(selected_feat)))"
      ],
      "metadata": {
        "id": "u82T4HEdQH71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=input[selected_feat]"
      ],
      "metadata": {
        "id": "i2lCIlcqQKOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Validation Set\n",
        "Finally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers."
      ],
      "metadata": {
        "id": "uVS7HjBzjKAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import train_test_split from sklearn library to make split of data into train sets and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input, target, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "Fz1MG0IFjPWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "train_inputs"
      ],
      "metadata": {
        "id": "GHJr-XzE3P47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "train_targets"
      ],
      "metadata": {
        "id": "Beb7sLS83SiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "val_inputs"
      ],
      "metadata": {
        "id": "UAVwiPms3W-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "val_targets"
      ],
      "metadata": {
        "id": "TTPqItJq3Yyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "NTmDLg4xZhfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "id": "OJ3cj594bbva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177e6305-a159-421c-e8ba-a9cf6562dac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 580 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.2.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn import neighbors\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "import lightgbm \n",
        "from catboost import CatBoostRegressor \n",
        "from sklearn.neural_network import MLPRegressor"
      ],
      "metadata": {
        "id": "rlchHs4bbHdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model that we are going to use \n",
        "models = [\n",
        "           ['LinearRegression: ',              LinearRegression()],\n",
        "           ['Lasso: ',                         Lasso()],\n",
        "           ['Ridge: ',                         Ridge()],\n",
        "           ['KNeighborsRegressor: ',           neighbors.KNeighborsRegressor()],\n",
        "           ['SVR:' ,                           SVR(kernel='rbf')],\n",
        "           ['DecisionTree ',                   DecisionTreeRegressor(random_state=42)],\n",
        "           ['RandomForest ',                   RandomForestRegressor(random_state=42)],\n",
        "           ['ExtraTreeRegressor :',            ExtraTreesRegressor(random_state=42)],\n",
        "           ['GradientBoostingRegressor: ',     GradientBoostingRegressor(random_state=42)],\n",
        "           ['XGBRegressor: ',                  xgb.XGBRegressor(random_state=42)] ,\n",
        "           ['Light-GBM: ',                     lightgbm.LGBMRegressor(num_leaves=41, n_estimators=200,random_state=42)],\n",
        "           ['CatBoost: ',                      CatBoostRegressor(verbose=0, early_stopping_rounds=10,random_state=42)],\n",
        "           ['MLPRegressor: ',                  MLPRegressor(  activation='relu', solver='adam',learning_rate='adaptive',max_iter=1000,learning_rate_init=0.01,alpha=0.01)]\n",
        "         ]"
      ],
      "metadata": {
        "id": "muXtd1iKbKPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all the proposed models and update the information in a list model_data\n",
        "import time\n",
        "from math import sqrt\n",
        "from sklearn import metrics\n",
        "\n",
        "model_data = []\n",
        "for name,model in models :\n",
        "\n",
        "    model_data_dic = {}\n",
        "    model_data_dic[\"Name\"] = name\n",
        "\n",
        "    start = time.time()\n",
        "    end = time.time()\n",
        "\n",
        "    model.fit(train_inputs,train_targets)\n",
        "    \n",
        "    model_data_dic[\"Train_Time\"] = end - start\n",
        "    # Training set\n",
        "    model_data_dic[\"Train_R2_Score\"] = metrics.r2_score(train_targets,model.predict(train_inputs))\n",
        "    model_data_dic[\"Train_RMSE_Score\"] = metrics.mean_squared_error(train_targets,model.predict(train_inputs),squared=False)\n",
        "    # Validation set\n",
        "    model_data_dic[\"Test_R2_Score\"] = metrics.r2_score(val_targets,model.predict(val_inputs))\n",
        "    model_data_dic[\"Test_RMSE_Score\"] = metrics.mean_squared_error(val_targets,model.predict(val_inputs),squared=False)\n",
        "\n",
        "    model_data.append(model_data_dic)"
      ],
      "metadata": {
        "id": "8dLM6uv_bMzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list to dataframe\n",
        "df = pd.DataFrame(model_data)\n",
        "df"
      ],
      "metadata": {
        "id": "XkJudmlObPYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot(x=\"Name\", y=['Test_R2_Score' , 'Train_R2_Score' , 'Test_RMSE_Score'], kind=\"bar\" , title = 'R2 Score Results' , figsize= (10,8)) ;"
      ],
      "metadata": {
        "id": "OeIA4Pe0bVhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Obervations\n",
        "1. Best results over test set are given by Extra Tree Regressor with R2 score of 0.57\n",
        "2. Least RMSE score is also by Extra Tree Regressor 0.65\n",
        "3. Lasso regularization over Linear regression was worst performing model"
      ],
      "metadata": {
        "id": "F39ta0CFbYpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import metrics from sklearn library\n",
        "from sklearn import metrics\n",
        "\n",
        "def evaluate_train(model, train_inputs,train_targets):\n",
        "    # Prediction on Train inputs\n",
        "    predictions = model.predict(train_inputs)\n",
        "    print('Train_Data- Model Performance')\n",
        "    print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(train_targets, predictions, squared=False))\n",
        "    print('R^2:', metrics.r2_score(train_targets, predictions))\n",
        "\n",
        "\n",
        "def evaluate_val(model, val_inputs,val_targets):\n",
        "    # Prediction on val inputs\n",
        "    predictions = model.predict(val_inputs)\n",
        "    print('Validation_data-Model Performance')\n",
        "    print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(val_targets, predictions, squared=False))\n",
        "    print('R^2:', metrics.r2_score(val_targets, predictions))   \n"
      ],
      "metadata": {
        "id": "kBjQYYUrcXwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 - Training a Linear Regression Model\n",
        "\n",
        "We're now ready to train the model. Linear regression is a commonly used technique for solving [regression problems](https://jovian.ai/aakashns/python-sklearn-logistic-regression/v/66#C6). In a linear regression model, the target is modeled as a linear combination (or weighted sum) of input features. The predictions from the model are evaluated using a loss function like the Root Mean Squared Error (RMSE).\n",
        "\n",
        "\n",
        "Here's a visual summary of how a linear regression model is structured:\n",
        "\n",
        "<img src=\"https://i.imgur.com/iTM2s5k.png\" width=\"480\">\n",
        "\n",
        "However, linear regression doesn't generalize very well when we have a large number of input columns with co-linearity i.e. when the values one column are highly correlated with values in other column(s). This is because it tries to fit the training data perfectly. \n",
        "\n",
        "Instead, we'll use Ridge Regression, a variant of linear regression that uses a technique called L2 regularization to introduce another loss term that forces the model to generalize better. Learn more about ridge regression here: https://www.youtube.com/watch?v=Q81RR3yKn30"
      ],
      "metadata": {
        "id": "ofZhV2cSaUX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        ">  Create and train a linear regression model using `sklearn.linear_model`."
      ],
      "metadata": {
        "id": "3ybsOl4veNPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "# Create the model\n",
        "LR= LinearRegression()\n",
        "# Fit the model using inputs and targets\n",
        "LR.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "ykrKbOYUeMjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**\n",
        "\n",
        "The model is now trained, and we can use it to generate predictions for the training and validation inputs. We can evaluate the model's performance using the RMSE (root mean squared error) loss function."
      ],
      "metadata": {
        "id": "01sfQ7dReWpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(LR, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "YenlKKSregbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(LR, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "X8J1xEV8eiuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation of model performance on test set\n",
        "import matplotlib.pyplot as plt\n",
        "fig=plt.figure(figsize=(10,8))\n",
        "plt.plot(10**(LR.predict(val_inputs)))\n",
        "plt.plot(np.array(10**val_targets), color='red')"
      ],
      "metadata": {
        "id": "PaiT3Frb-1Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance for LinearRegression\n",
        "\n",
        "\n",
        "Let's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important."
      ],
      "metadata": {
        "id": "d89v9NyplSKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 11**: Identify the weights (or coefficients) assigned to for different features by the model.\n",
        "> \n",
        "> *Hint:* Read [the docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
      ],
      "metadata": {
        "id": "FjTOj_Oy4bdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important features or coefficient of the model\n",
        "weights = model.coef_.flatten()"
      ],
      "metadata": {
        "id": "-gH7_W7tlTOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a dataframe to view the weight assigned to each column."
      ],
      "metadata": {
        "id": "u4xCHhPDlYcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_df = pd.DataFrame({\n",
        "    'columns': train_inputs.columns,\n",
        "    'weight': weights\n",
        "}).sort_values('weight', ascending=False)"
      ],
      "metadata": {
        "id": "5qOJx4tPlavS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Feature Importance on bar plot\n",
        "plt.title('Feature Importance')\n",
        "sns.barplot(data=weights_df.head(10), x='weight', y='columns');"
      ],
      "metadata": {
        "id": "g6NZqbOeld_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the training accuracy is 100%, the accuracy on the validation set is just about 76%."
      ],
      "metadata": {
        "id": "rU_ldrpeRicK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso Regression"
      ],
      "metadata": {
        "id": "VBRtMM5Y_q5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lasso=Lasso()\n",
        "parameters={'alpha':[1e-15,1e-13,1e-10,1e-8,1e-6,1e-4,1e-3, 1e-2,1e-1,1,5,10,20,30,40,50,60,100]}\n",
        "las_regressor=GridSearchCV(lasso,parameters, scoring= 'neg_mean_squared_error',cv=5)\n",
        "las_regressor.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "7GlCwIGq_mWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('using', las_regressor.best_params_,'the negative mean squared error ', las_regressor.best_score_)"
      ],
      "metadata": {
        "id": "WcvdTJDHAUMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(las_regressor, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "IrJ30x77AaAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(las_regressor, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "ss0uBq-ZAdGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation of model performance on test set\n",
        "import matplotlib.pyplot as plt\n",
        "fig=plt.figure(figsize=(10,8))\n",
        "plt.plot(10**(las_regressor.predict(val_inputs)))\n",
        "plt.plot(np.array(10**val_targets), color='red')"
      ],
      "metadata": {
        "id": "Le8PzcVCAjJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ridge Regression"
      ],
      "metadata": {
        "id": "Luz-eWzNAzIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rid=Ridge()\n",
        "parameters={'alpha': [1e-15,1e-13,1e-10,1e-8,1e-6,1e-4,1e-3, 1e-2,1e-1,1,5,10,20,30,40,50,60,100]}\n",
        "rid_regressor=GridSearchCV(rid,parameters,scoring='neg_mean_squared_error', cv=5)\n",
        "rid_regressor.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "F3qciZMSAuaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('using', rid_regressor.best_params_,'the negative mean squared error ', rid_regressor.best_score_)"
      ],
      "metadata": {
        "id": "jgHVA3UdA4ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(rid_regressor, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "RIfG2iaUA_Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(rid_regressor, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "CunLm4STBBbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation of model performance on test set\n",
        "import matplotlib.pyplot as plt\n",
        "fig=plt.figure(figsize=(10,8))\n",
        "plt.plot(10**(las_regressor.predict(val_inputs)))\n",
        "plt.plot(np.array(10**val_targets), color='red')"
      ],
      "metadata": {
        "id": "XkZUyP2IA9Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ElasticNet Regressor"
      ],
      "metadata": {
        "id": "8X7gjbrfBKfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "elastic= ElasticNet()\n",
        "parameters={'alpha': [1e-15,1e-13,1e-10,1e-8,1e-6,1e-4,1e-3, 1e-2,1e-1,1,5,10,20,30,40,50,60,100], 'l1_ratio': [0.3,0.4,0.5,0.6,0.7,0.8]}            \n",
        "elastic_reg =GridSearchCV(elastic,parameters,scoring='neg_mean_squared_error', cv=5)\n",
        "elastic_reg.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "lLWQ1GssBItL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('the best values are', elastic_reg.best_params_, 'having the minimum error', elastic_reg.best_score_)"
      ],
      "metadata": {
        "id": "3u_VTahgBWa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(elastic_reg, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "cMbOuK0-BYQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(elastic_reg, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "zQ9HKS5TBagk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation of model performance on test set\n",
        "import matplotlib.pyplot as plt\n",
        "fig=plt.figure(figsize=(10,8))\n",
        "plt.plot(10**(elastic_reg.predict(val_inputs)))\n",
        "plt.plot(np.array(10**val_targets), color='red')"
      ],
      "metadata": {
        "id": "3KYUJtSTBgKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat= {  'model':['LinearRegression','Lasso','Ridge','Lasso_CV','Ridge_CV','Elastic Net'],\n",
        "        'score':[LR_score,las_score,reg_score,las_reg_score, rid_reg_score, elastic_reg_score], \n",
        "        'MSE': [LR_MSE,las_MSE,reg_MSE,las_reg_MSE, rid_reg_MSE, elastic_reg_MSE],\n",
        "        'RMSE': [np.sqrt(LR_MSE),np.sqrt(las_MSE),np.sqrt(reg_MSE),np.sqrt(las_reg_MSE), np.sqrt(rid_reg_MSE),np.sqrt(elastic_reg_MSE)],\n",
        "        'r2_score' : [LR_r2, las_r2, reg_r2,las_reg_r2, rid_reg_r2, elastic_reg_r2],\n",
        "        'adjR2': [LR_ra2, las_ra2, reg_ra2,las_reg_ra2, rid_reg_ra2, elastic_reg_ra2]\n",
        "       }\n",
        "\n",
        "ff=pd.DataFrame(dat)       "
      ],
      "metadata": {
        "id": "FDLD0Ua4BlTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 -Training a SupportVector"
      ],
      "metadata": {
        "id": "4wOT0FQt6BEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* most important SVR parameter is Kernel type. It can be \n",
        "1. linear\n",
        "2. polynomial  \n",
        "3. gaussian SVR\n",
        "\n",
        " We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel."
      ],
      "metadata": {
        "id": "YhqIBaga8FLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "SVR_model = SVR(kernel='rbf')\n",
        "SVR_model.fit(X,y)"
      ],
      "metadata": {
        "id": "U4BHCGhP8Aek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(SVR_model, train_inputs,train_targets)\n",
        "evaluate_val(SVR_model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "CI5ERd3T89Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3 -Training a KNN"
      ],
      "metadata": {
        "id": "zto9575G9FPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neighbors\n",
        "\n",
        "KNR = neighbors.KNeighborsRegressor()\n",
        "KNR.fit(train_inputs,train_targets)  #fit the model\n",
        "\n",
        "evaluate_train(KNR, train_inputs,train_targets)\n",
        "evaluate_val(KNR, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "BKaOVN5B9Eg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4 -Training and Visualizing Decision Trees\n",
        "\n",
        "A decision tree in general parlance represents a hierarchical series of binary decisions:\n",
        "\n",
        "<img src=\"https://i.imgur.com/qSH4lqz.png\" width=\"480\">\n",
        "\n",
        "A decision tree in machine learning works in exactly the same way, and except that we let the computer figure out the optimal structure & hierarchy of decisions, instead of coming up with criteria manually."
      ],
      "metadata": {
        "id": "K43waxrTahnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "We can use `DecisionTreeRegressor` from `sklearn.tree` to train a decision tree."
      ],
      "metadata": {
        "id": "65e1cB4qeyws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create the model\n",
        "DRT=DecisionTreeRegressor(random_state=42)\n",
        "# Fit the model\n",
        "DRT.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "bLG9nHRHe2Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "\n",
        "Let's evaluate the decision tree using the accuracy score."
      ],
      "metadata": {
        "id": "NRZx2HFPe4yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(DRT, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "K2yC0aW1e4YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set accuracy is close to 100%! But we can't rely solely on the training set accuracy, we must evaluate the model on the validation set too. \n",
        "\n",
        "We can make predictions and compute accuracy in one step using `model.score`"
      ],
      "metadata": {
        "id": "VXp917dUe9rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(DRT, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "vRSXw07EfAY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the training accuracy is 100%, the accuracy on the validation set is just about 76%."
      ],
      "metadata": {
        "id": "iXQqAnXNfDPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization\n",
        "\n",
        "We can visualize the decision tree _learned_ from the training data."
      ],
      "metadata": {
        "id": "Z5XrW_fPRfUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import plot_tree from sklearn library\n",
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(80,20))\n",
        "plot_tree(model, feature_names=train_inputs.columns, max_depth=2, filled=True);"
      ],
      "metadata": {
        "id": "ktAsnTtiRf1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let's check the depth of the tree that was created."
      ],
      "metadata": {
        "id": "Na89OwLRRqgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.tree_.max_depth"
      ],
      "metadata": {
        "id": "Ut_ppgFtRrfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also display the tree as text, which can be easier to follow for deeper trees."
      ],
      "metadata": {
        "id": "dgBOrhIHR5_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import export_text from sklearn library\n",
        "from sklearn.tree import export_text\n",
        "tree_text = export_text(model, max_depth=10, feature_names=list(train_inputs.columns))\n",
        "print(tree_text[:3000])"
      ],
      "metadata": {
        "id": "kwncIcO9R_gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance\n",
        "\n",
        "\n",
        "Based on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree."
      ],
      "metadata": {
        "id": "iA4tR7eHSD6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important features or coefficient of the model\n",
        "model.feature_importances_"
      ],
      "metadata": {
        "id": "ipASFEwtSEgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's turn this into a dataframe and visualize the most important features."
      ],
      "metadata": {
        "id": "vkmPx7wySH-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df = pd.DataFrame({\n",
        "    'feature': train_inputs.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)"
      ],
      "metadata": {
        "id": "U60T_xE_SIel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Feature Importance on bar plot\n",
        "plt.title('Feature Importance')\n",
        "sns.barplot(data=importance_df.head(10), x='importance', y='feature');"
      ],
      "metadata": {
        "id": "Nh7UTF01SNzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5-Training a Random Forest\n",
        "\n",
        "While tuning the hyperparameters of a single decision tree may lead to some improvements, a much more effective strategy is to combine the results of several decision trees trained with slightly different parameters. This is called a random forest model. \n",
        "\n",
        "The key idea here is that each decision tree in the forest will make different kinds of errors, and upon averaging, many of their errors will cancel out. This idea is also commonly known as the \"wisdom of the crowd\":\n",
        "\n",
        "<img src=\"https://i.imgur.com/4Dg0XK4.png\" width=\"480\">"
      ],
      "metadata": {
        "id": "yNmVCdH0ShzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "We can use `RandomForestRegressor` from `sklearn.ensemble` to train a decision tree."
      ],
      "metadata": {
        "id": "xSyvtYPBfaON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Create the model\n",
        "RF = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "# Fit the model\n",
        "RF.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "r-Yuk9CvfcNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`n_jobs` allows the random forest to use mutiple parallel workers to train decision trees, and `random_state=42` ensures that the we get the same results for each execution."
      ],
      "metadata": {
        "id": "hom80baxffCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "\n",
        "Let's evaluate the Random Forest using the accuracy score."
      ],
      "metadata": {
        "id": "2tigRThVfiEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(RF, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "ioKyc71hfewX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(RF, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "PdiLsfOgfkjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, the training accuracy is almost 100%, but this time the validation accuracy is much better. In fact, it is better than the best single decision tree we had trained so far. Do you see the power of random forests?\n",
        "\n",
        "This general technique of combining the results of many models is called \"ensembling\", it works because most errors of individual models cancel out on averaging. Here's what it looks like visually:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFgEjcgxfo-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning using Optuna"
      ],
      "metadata": {
        "id": "osWzhNsYJeFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def objective(trial):\n",
        "    criterion = trial.suggest_categorical('criterion', ['mse', 'mae'])\n",
        "    bootstrap = trial.suggest_categorical('bootstrap',['True','False'])\n",
        "    max_depth = trial.suggest_int('max_depth', 1, 500)\n",
        "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt','log2'])\n",
        "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 500)\n",
        "    n_estimators =  trial.suggest_int('n_estimators', 30, 1000)\n",
        "    \n",
        "    regr = RandomForestRegressor(bootstrap = bootstrap, criterion = criterion,\n",
        "                                 max_depth = max_depth, max_features = max_features,\n",
        "                                 max_leaf_nodes = max_leaf_nodes,n_estimators = n_estimators,n_jobs=2)\n",
        "    \n",
        "    \n",
        "    regr.fit(train_inputs, train_targets)\n",
        "    y_pred = regr.predict(val_inputs)\n",
        "    return r2_score(val_targets, y_pred)\n",
        "    "
      ],
      "metadata": {
        "id": "TlDYMqrgWgbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute optuna and set hyperparameters\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100,show_progress_bar = True)"
      ],
      "metadata": {
        "id": "cQZQLZC6Jo-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance with tuned hyperparameters\n",
        "optimised_rf = RandomForestRegressor(bootstrap = study.best_params['bootstrap'], criterion = study.best_params['criterion'],\n",
        "                                     max_depth = study.best_params['max_depth'], max_features = study.best_params['max_features'],\n",
        "                                     max_leaf_nodes = study.best_params['max_leaf_nodes'],n_estimators = study.best_params['n_estimators'],\n",
        "                                     n_jobs=2)\n",
        "#learn\n",
        "optimised_rf.fit(train_inputs, train_targets)\n"
      ],
      "metadata": {
        "id": "YRpQ1xzpbWTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "id": "Lr7W2EypbhOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_optimization_history(study)\n",
        "optuna.visualization.plot_slice(study)"
      ],
      "metadata": {
        "id": "g74e_ZCFJxwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = RandomForestRegressor(random_state=42)\n",
        "base_model.fit(train_inputs, train_targets)\n",
        "base_accuracy = evaluate_val(base_model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "b0RtBnV9XF2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimised_accuracy = evaluate_val(optimised_rf, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "-GO7HsoVXH30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Just like decision trees, random forests also have several hyperparameters. In fact many of these hyperparameters are applied to the underlying decision trees. \n",
        "\n",
        "Let's study some the hyperparameters for random forests. You can learn more about them here."
      ],
      "metadata": {
        "id": "94V2O8MWHVvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomSearch\n",
        "As the name suggests the RandomSearch algorithm tries random combinations of a range\n",
        "of values of given parameters. The numerical parameters can be specified as a range\n",
        "(unlike fixed values in GridSearch). You can control the number of iterations of random\n",
        "searches that you would like to perform. It is known to find a very good combination in a\n",
        "lot less time compared to GridSearch; however you have to carefully choose the range for\n",
        "parameters and the number of random search iteration as it can miss the best parameter\n",
        "combination with lesser iterations or smaller ranges.\n",
        "Let’s try the RandomSearchCV for same combination that we tried for GridSearch\n",
        "and compare the time / accuracy."
      ],
      "metadata": {
        "id": "BuhAvc4uRqd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "37Q0qcFOox93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
        "# Create the random grid\n",
        "random_grid ={'bootstrap': [True, False],\n",
        "              'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "              'max_features': ['auto', 'sqrt'],\n",
        "              'min_samples_leaf': [1, 2, 4],\n",
        "              'min_samples_split': [2, 5, 10],\n",
        "              'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
        "\n",
        "RF = RandomForestRegressor(n_jobs=-1, random_state=42)              \n",
        "\n",
        "# Random search of parameters, using 3 fold cross validation,\n",
        "random_search = RandomizedSearchCV(estimator = RF, \n",
        "                                   param_distributions = random_grid, \n",
        "                                   n_iter = 100, cv = 3, verbose=2)\n",
        "# Fit the random search model\n",
        "random_search.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "rDEaH7V5Hat4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can view the best parameters from fitting the random search\n",
        "best_random_search= random_search.best_params_"
      ],
      "metadata": {
        "id": "q5jpbqfnIF-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "base_model.fit(train_features, train_labels)\n",
        "base_accuracy = evaluate_train(model, val_inputs, val_targets)"
      ],
      "metadata": {
        "id": "k5ikHwDt3DZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_random = random_search.best_estimator_\n",
        "random_accuracy = evaluate(best_random, val_inputs, val_targets)"
      ],
      "metadata": {
        "id": "6FmiTrkTP5am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
      ],
      "metadata": {
        "id": "CWGtAvrSP7PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearch**\n",
        "\n",
        "For a given model, you can define a set of parameter values that you would like to try.\n",
        "Then using the GridSearchCV function of scikit-learn, models are built for all possible\n",
        "combinations of a preset list of values of hyperparameter provided by you, and the best\n",
        "combination is chosen based on the cross-validation score. There are two disadvantages\n",
        "associated with GridSearchCV.\n",
        "1. Computationally expensive:\n",
        "2. Not perfect optimal but nearly optimal parameters:"
      ],
      "metadata": {
        "id": "UyoJ8_yTRizC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
        "\n",
        "# Create the parameter grid based on the results of random search \n",
        "search_grid = {'bootstrap': [True],\n",
        "              'max_depth': [80, 90, 100, 110],\n",
        "              'max_features': [2, 3],\n",
        "              'min_samples_leaf': [3, 4, 5],\n",
        "              'min_samples_split': [8, 10, 12],\n",
        "              'n_estimators': [100, 200, 300, 1000] }\n",
        "\n",
        "RF = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = RF, \n",
        "                           param_grid = search_grid, \n",
        "                           cv = kfold, verbose = 2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(train_inputs, train_targets)             "
      ],
      "metadata": {
        "id": "UgahiiHiR-uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the best parameters from fitting the Grid search\n",
        "best_grid_search = grid_search.best_params_"
      ],
      "metadata": {
        "id": "g5oxIRkBSCYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the best estimator from fitting the Grid search\n",
        "best_grid_estimator = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "G08UDNlhTx1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_accuracy = evaluate(best_grid_search, val_inputs, val_targets)"
      ],
      "metadata": {
        "id": "lDhR177wSyfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
      ],
      "metadata": {
        "id": "2LI7LN7KTcyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Best Model with custom Hyperparameters\n",
        "\n",
        ">  Train a random forest regressor model with the best hyperparameters to minimize the validation loss."
      ],
      "metadata": {
        "id": "qJnUnKBjZyfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model with custom hyperparameters\n",
        "model = RandomForestRegressor(random_state=42, n_jobs=-1,max_depth=20,\n",
        "                                    max_features=0.7,n_estimators=40)\n",
        "# Fit the model\n",
        "model.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "Outvotm5Z6N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "evaluate_train(best_grid_estimator, train_inputs,train_targets)\n",
        "\n",
        "# Validation set\n",
        "evaluate_val(best_grid_estimator, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "I7Plc8ppZ_U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization\n",
        "\n",
        "We can visualize the decision tree _learned_ from the training data.\n",
        ">We can can access individual decision trees using `model.estimators_`"
      ],
      "metadata": {
        "id": "D0Lwlv4LS7Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.estimators_[0]"
      ],
      "metadata": {
        "id": "0DcGwvk-S0ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import plot_tree from sklearn library\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plt.figure(figsize=(80,20))\n",
        "plot_tree(model.estimators_[0], max_depth=2, feature_names=train_inputs.columns, filled=True, rounded=True, );"
      ],
      "metadata": {
        "id": "L3DvTlWwTCNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance\n",
        "Just like decision tree, random forests also assign an \"importance\" to each feature, by combining the importance values from individual trees."
      ],
      "metadata": {
        "id": "JVDugkyTTEqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important features or coefficient of the model\n",
        "model.feature_importances_"
      ],
      "metadata": {
        "id": "wDLD9MlfTHf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's turn this into a dataframe and visualize the most important features."
      ],
      "metadata": {
        "id": "TLZDZynNdS8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df = pd.DataFrame({\n",
        "    'feature': train_inputs.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)"
      ],
      "metadata": {
        "id": "S6s0W3AwTNHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Feature Importance on bar plot\n",
        "plt.title('Feature Importance')\n",
        "sns.barplot(data=importance_df.head(10), x='importance', y='feature');"
      ],
      "metadata": {
        "id": "WFxTZ2X1TPID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 6 ExtraTreeRegressor"
      ],
      "metadata": {
        "id": "oFQTTaveAYLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "ijOJs5gVDRZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "num_trees = 100\n",
        "# Create the model\n",
        "ExtraTree = ExtraTreesRegressor(n_estimators=num_trees)\n",
        "# Fit the model\n",
        "ExtraTree.fit(train_inputs,train_targets)\n"
      ],
      "metadata": {
        "id": "SUVihAAPAY87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evalute"
      ],
      "metadata": {
        "id": "a2Vc5DgUDPKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(ExtraTree, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "Wy9Wr6mRDKR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(ExtraTree, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "tV8PhO64DMqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning using optuna\n"
      ],
      "metadata": {
        "id": "sGORtn4PDTPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def objective(trial):\n",
        "    criterion = trial.suggest_categorical('criterion', ['mse', 'mae'])\n",
        "    bootstrap = trial.suggest_categorical('bootstrap',['True','False'])\n",
        "    max_depth = trial.suggest_int('max_depth', 1, 500)\n",
        "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt','log2'])\n",
        "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 500)\n",
        "    n_estimators =  trial.suggest_int('n_estimators', 30, 1000)\n",
        "    \n",
        "    regr = ExtraTreeRegressor(bootstrap = bootstrap, criterion = criterion,\n",
        "                                 max_depth = max_depth, max_features = max_features,\n",
        "                                 max_leaf_nodes = max_leaf_nodes,n_estimators = n_estimators,n_jobs=2)\n",
        "    \n",
        "    \n",
        "    regr.fit(train_inputs, train_targets)\n",
        "    y_pred = regr.predict(val_inputs)\n",
        "    return r2_score(val_targets, y_pred)\n",
        "    "
      ],
      "metadata": {
        "id": "PwKUpkLBUPQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute optuna and set hyperparameters\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100,show_progress_bar = True)"
      ],
      "metadata": {
        "id": "2KfPVIP9UVVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance with tuned hyperparameters\n",
        "optimised_Et = ExtraTreeRegressor(bootstrap = study.best_params['bootstrap'], criterion = study.best_params['criterion'],\n",
        "                                     max_depth = study.best_params['max_depth'], max_features = study.best_params['max_features'],\n",
        "                                     max_leaf_nodes = study.best_params['max_leaf_nodes'],n_estimators = study.best_params['n_estimators'],\n",
        "                                     n_jobs=2)\n",
        "#learn\n",
        "optimised_Et.fit(train_inputs, train_targets)\n"
      ],
      "metadata": {
        "id": "CFPhdIZ1czsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "id": "k716c9HGUf0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "lTwxUJp4UiDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_slice(study)"
      ],
      "metadata": {
        "id": "pKSySrPOUjwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ExtraTreesRegressor(random_state=42)\n",
        "base_model.fit(train_inputs, train_targets)\n",
        "base_accuracy = evaluate_val(base_model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "ddQzN16rUn3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimised_accuracy = evaluate_val(optimised_Et, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "y7VSt2-XUqhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = [{\n",
        "              'max_depth': [80, 150, 200,250],\n",
        "              'n_estimators' : [100,150,200,250],\n",
        "              'max_features': [\"auto\", \"sqrt\", \"log2\"]\n",
        "            }]\n",
        "reg = ExtraTreesRegressor(random_state=40)\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = reg, param_grid = param_grid, cv = 5, n_jobs = -1 , scoring='r2' , verbose=2)\n",
        "grid_search.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "DTKlLVa6DP-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuned parameter set\n",
        "best_grid_search=grid_search.best_params_"
      ],
      "metadata": {
        "id": "aFDJG_NbEQA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best possible parameters for ExtraTreesRegressor\n",
        "grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "leFjjlK7ESrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_accuracy = evaluate(best_grid_search,  val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "MisJHjqtEvbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 7- Training a Gradient Boosting Machines (GBMs) with XGBoost"
      ],
      "metadata": {
        "id": "ez6buitWUiFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is a type of ensemble learning that uses the previous model's result as an input to the next one. Instead of training models separately, boosting trains models sequentially, each new model being trained to correct the errors of the previous ones. At each iteration (round), the outcomes predicted correctly are given a lower weight, and the ones wrongly predicted a higher weight. It then uses a weighted average to produce a final outcome.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*PZd-TOxSLV_--3glkFHwxQ.png\" width=\"600\">"
      ],
      "metadata": {
        "id": "ezOav-YlU8v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to train our gradient boosting machine (GBM) model. Here's how a GBM model works:\n",
        "\n",
        "1. The average value of the target column and uses as an initial prediction every input.\n",
        "2. The residuals (difference) of the predictions with the targets are computed.\n",
        "3. A decision tree of limited depth is trained to **predict just the residuals** for each input.\n",
        "4. Predictions from the decision tree are scaled using a parameter called the learning rate (this prevents overfitting)\n",
        "5. Scaled predictions fro the tree are added to the previous predictions to obtain the new and improved predictions.\n",
        "6. Steps 2 to 5 are repeated to create new decision trees, each of which is trained to predict just the residuals from the previous prediction.\n",
        "\n",
        "The term \"gradient\" refers to the fact that each decision tree is trained with the purpose of reducing the loss from the previous iteration (similar to gradient descent). The term \"boosting\" refers the general technique of training new models to improve the results of an existing model. \n",
        "\n",
        "Here's a visual representation of gradient boosting:\n",
        "\n",
        "![](https://miro.medium.com/max/560/1*85QHtH-49U7ozPpmA5cAaw.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "3cke0jYclMrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "To train a GBM, we can use the `XGBRegressor` class from the [`XGBoost`](https://xgboost.readthedocs.io/en/latest/) library."
      ],
      "metadata": {
        "id": "i7kQZ0dwlS_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import XGBRegressor from xgboost \n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# create the model\n",
        "model = XGBRegressor(random_state=42, n_jobs=-1)\n",
        "# Fit the model\n",
        "model.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "areC81jvlN8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Evaluation\n",
        "\n",
        "Note that when using the Learning API you can input and access an evaluation metric, whereas when using the Scikit-learn API you have to calculate it."
      ],
      "metadata": {
        "id": "1tZvreaSlXTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(model, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "9Wg4fdrolX8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "vuXkvhYrPHF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning using Optuna"
      ],
      "metadata": {
        "id": "p2yjFw2LKNU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def objective(trial):\n",
        "  \n",
        "    param = {\n",
        "  \n",
        "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
        "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
        "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
        "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
        "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018, 0.02]),\n",
        "        'n_estimators': 10000,\n",
        "        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17]),\n",
        "        'random_state': trial.suggest_categorical('random_state', [42]),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
        "    }\n",
        "    model = xgb.XGBRegressor(**param)  \n",
        "    \n",
        "    model.fit(train_inputs,train_targets,eval_set=[(val_inputs,val_targets)],early_stopping_rounds=100,verbose=False)\n",
        "    \n",
        "    preds = model.predict(train_inputs)  \n",
        "    rmse = mean_squared_error(train_targets, preds,squared=False)\n",
        "    \n",
        "    return rmse"
      ],
      "metadata": {
        "id": "Bc8RvoHaKSp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=30,show_progress_bar = True)"
      ],
      "metadata": {
        "id": "KKXa8WHvdW9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "id": "hTDCpfRzLXVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\n",
        "optuna.visualization.plot_optimization_history(study)\n",
        "#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\n",
        "optuna.visualization.plot_parallel_coordinate(study)\n",
        "#Visualize parameter importances.\n",
        "optuna.visualization.plot_param_importances(study)"
      ],
      "metadata": {
        "id": "t83sBn2DLcoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = xgb.XGBRegressor(random_state=42)\n",
        "base_model.fit(train_inputs, train_targets)\n",
        "base_accuracy = evaluate_val(base_model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "-gDRFYjPLfZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_model = xgb.XGBRegressor(**study.best_params,random_state=42)\n",
        "tune_model.fit(train_inputs, train_targets)\n",
        "tune_accuracy = evaluate_val(tune_model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "NihY9v6iYPp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning and Regularization\n",
        "\n",
        "Objective function\n",
        "XGBoost is a great choice in multiple situations, including regression and classification problems. Based on the problem and how you want your model to learn, you’ll choose a different objective function.\n",
        "\n",
        "The most commonly used are:\n",
        "\n",
        "* reg:squarederror: for linear regression\n",
        "* reg:logistic: for logistic regression\n",
        "* binary:logistic: for logistic regression — with output of the probabilities"
      ],
      "metadata": {
        "id": "khUNihlnllHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Random Search with Cross Validation`"
      ],
      "metadata": {
        "id": "sEwxRwcTlqPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the random grid\n",
        "random_grid = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
        "                'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "                'subsample': [0.5,0.6,0.7,0.8,0.9 1.0]\n",
        "                'colsample_bytree': [0.4,0.5,0.6,0.7,0.8,0.9 1.0]\n",
        "                'colsample_bylevel': [0.4,0.5,0.6,0.7,0.8,0.9 1.0]\n",
        "                'n_estimators': [100, 500, 1000] }\n",
        "                \n",
        "# Random search of parameters, using 3 fold cross validation,\n",
        "random_search = RandomizedSearchCV(estimator = model, \n",
        "                               param_distributions = random_grid, \n",
        "                               n_iter = 100, cv = 3, verbose=2)\n",
        "# Fit the random search model\n",
        "random_search.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "9dIf10SjllsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can view the best parameters from fitting the random search\n",
        "best_random_search= random_search.best_params_"
      ],
      "metadata": {
        "id": "kyFrFLSraPFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "base_model.fit(train_features, train_labels)\n",
        "base_accuracy = evaluate_train(model,val_inputs, val_targets)"
      ],
      "metadata": {
        "id": "AuY4quqTavLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_random = random_search.best_estimator_\n",
        "random_accuracy = evaluate(best_random, val_inputs, val_targets)"
      ],
      "metadata": {
        "id": "ErJHIt8Rau5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
      ],
      "metadata": {
        "id": "YMbfDEYoFsrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Grid Search with Cross Validation`"
      ],
      "metadata": {
        "id": "AxKgCU-5a5vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the parameter grid based on the results of random search \n",
        "search_grid = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
        "                'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "                'subsample': [0.5,0.6,0.7,0.8,0.9 1.0]\n",
        "                'colsample_bytree': [0.4,0.5,0.6,0.7,0.8,0.9 1.0]\n",
        "                'colsample_bylevel': [0.4,0.5,0.6,0.7,0.8,0.9 1.0]\n",
        "                'n_estimators': [100, 500, 1000] }\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = model, \n",
        "                           param_grid = search_grid, \n",
        "                           cv = 3, verbose = 2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "UXebOx_3auvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the best parameters from fitting the Grid search\n",
        "best_grid_search = grid_search.best_params_"
      ],
      "metadata": {
        "id": "oXhwfHt-bl7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the best estimator from fitting the Grid search\n",
        "best_grid_estimator = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "wTRJuOkHbx3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_accuracy = evaluate(best_grid_search, val_inputs, val_targets)"
      ],
      "metadata": {
        "id": "ijIOTH7UbxqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
      ],
      "metadata": {
        "id": "UCOVQvsYb4bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Best Model with custom Hyperparameters\n",
        "\n",
        ">  Train a random forest regressor model with the best hyperparameters to minimize the validation loss."
      ],
      "metadata": {
        "id": "QDhsTLO7b9z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBRegressor(n_jobs=-1, random_state=42, n_estimators=1700, \n",
        "                     learning_rate=0.3, max_depth=7, subsample=0.9, \n",
        "                     colsample_bytree=0.7)\n",
        "# Fit the model\n",
        "model.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "ENRyN-OHfgpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "evaluate_train(best_grid_estimator, train_inputs,train_targets)\n",
        "\n",
        "# Validation set\n",
        "evaluate_val(best_grid_estimator, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "UJ0xTHX4fpAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 8 - Light GBM \n",
        "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
        "\n",
        "* Faster training speed and higher efficiency.\n",
        "* Lower memory usage.\n",
        "* Better accuracy.\n",
        "* Support of parallel, distributed, and GPU learning.\n",
        "* Capable of handling large-scale data."
      ],
      "metadata": {
        "id": "2A6_rx1fhx6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "To train a LGBM, we can use the `LGBMRegressor` frm class lightgbm."
      ],
      "metadata": {
        "id": "4VScFOXHh2Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm \n",
        "# create the model\n",
        "LGBM=lightgbm.LGBMRegressor(random_state=42)\n",
        "# Fit the model\n",
        "LGBM.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "ZwFqKTbAfulp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "\n",
        "Let's evaluate the predictions using score."
      ],
      "metadata": {
        "id": "WrLkcpvIh8hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(LGBM, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "-GgnpGyGiEMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(LGBM, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "XDHQGGARh9N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization\n",
        "\n",
        "We can visualize the LGBM _learned_ from the training data."
      ],
      "metadata": {
        "id": "y0pxiyPsiHW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgb.plot_importance(model)"
      ],
      "metadata": {
        "id": "JHvPerpDiLPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb.plot_tree(model,figsize=(80,40))"
      ],
      "metadata": {
        "id": "S061LRToiXsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb.plot_metric(model)"
      ],
      "metadata": {
        "id": "szl4D_gKi4ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning with optuna"
      ],
      "metadata": {
        "id": "HS_VSO0ctN69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna import Trial, visualization\n",
        "from optuna.samplers import TPESampler\n",
        "def objective(trial,data=data):\n",
        "    \n",
        "   \n",
        "    param = {\n",
        "        'metric': 'rmse', \n",
        "        'random_state': 42,\n",
        "        'n_estimators': 10000,\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
        "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
        "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
        "        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n",
        "        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n",
        "        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
        "        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n",
        "    }\n",
        "\n",
        "    model = lightgbm.LGBMRegressor(**param)  \n",
        "    model.fit(train_inputs,train_targets,eval_set=[(val_inputs,val_targets)],early_stopping_rounds=100,verbose=False)\n",
        "    \n",
        "    preds = model.predict(val_inputs)\n",
        "    rmse = mean_squared_error(val_targets, preds,squared=False)\n",
        "    \n",
        "    return rmse"
      ],
      "metadata": {
        "id": "ueVwZws_ZUXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10,show_progress_bar = True)"
      ],
      "metadata": {
        "id": "ur6JNO6nZc6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's do some Quick Visualization for Hyperparameter Optimization Analysis\n",
        "* Optuna provides various visualization features in optuna.visualization to analyze optimization results visually"
      ],
      "metadata": {
        "id": "MIusLhrJZnRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\n",
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "qlTvmQq6Zk1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\n",
        "optuna.visualization.plot_parallel_coordinate(study)"
      ],
      "metadata": {
        "id": "R2SCXGBVZqoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = LGBMRegressor(random_state=42)\n",
        "base_model.fit(train_inputs, train_targets)\n",
        "base_accuracy = evaluate_val(base_model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "uWH385rdaFNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_model = LGBMRegressor(**study.best_params,random_state=42)\n",
        "tune_model.fit(train_inputs, train_targets)\n",
        "tune_accuracy = evaluate_val(tune_model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "OFPajzGFaNF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 9 - Catboost "
      ],
      "metadata": {
        "id": "8MUDyow-txrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "We can install CatBoost using the following command:\n"
      ],
      "metadata": {
        "id": "t8EeSiBit4vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "id": "Z2I8r_CutyRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "import catboost as cb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.inspection import permutation_importance"
      ],
      "metadata": {
        "id": "eoIp4ORct9gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pool Object\n",
        "* The Pool function in CatBoost combines independent and dependent variables (X and y), as well as categorical features.\n",
        "* We pass Pool Object as a training data to fit() method\n",
        "* We don’t need to define the “cat features” parameter separately when constructing the model since the pool object already has these details.\n",
        "We will create a pool object using the below code."
      ],
      "metadata": {
        "id": "fJgJkhbWuA3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = cb.Pool(train_inputs, train_targets) \n",
        "test_dataset = cb.Pool(val_inputs, val_targets)"
      ],
      "metadata": {
        "id": "gke6GRaruDA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cb.CatBoostRegressor(loss_function=\"RMSE\")"
      ],
      "metadata": {
        "id": "f4GobXQKuFVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = {'iterations': [100, 150, 200,300,400,500],\n",
        "        'learning_rate': [0.03, 0.1],\n",
        "        'depth': [2, 4, 6, 8],\n",
        "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
        "model.grid_search(grid, train_dataset)"
      ],
      "metadata": {
        "id": "0V5JGwGKuFDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "\n",
        "Let's evaluate the predictions using score."
      ],
      "metadata": {
        "id": "-vO-twN1uJrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(model, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "zHwA7QSpuKpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "MVHPGx4LuNz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning "
      ],
      "metadata": {
        "id": "9ZKiEORguYic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = cb.CatBoostRegressor(loss_function=\"RMSE\")\n",
        "grid={'depth': [8],\n",
        "  'iterations': [500],\n",
        "  'l2_leaf_reg': [0.5],\n",
        "  'learning_rate': [0.1]}\n",
        "model1.grid_search(grid, train_dataset)"
      ],
      "metadata": {
        "id": "o3k_gusvuan4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets tune it more"
      ],
      "metadata": {
        "id": "FdSLI7HzudPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = cb.CatBoostRegressor(loss_function=\"RMSE\")\n",
        "grid={'depth': [8],\n",
        "  'iterations': [1000],\n",
        "  'l2_leaf_reg': [0.01],\n",
        "  'learning_rate': [0.1]}\n",
        "model1.grid_search(grid, train_dataset)"
      ],
      "metadata": {
        "id": "dGmAE3wNufzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_train(model, train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "LSicmEonui3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_val(model, val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "G_cAJaBJulND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning with optuna"
      ],
      "metadata": {
        "id": "BKd84LM7c_Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna import Trial, visualization\n",
        "from optuna.samplers import TPESampler\n",
        "def objective(trial,data=data):\n",
        "    \n",
        "    param = {\n",
        "        'loss_function': 'RMSE',\n",
        "        'task_type': 'GPU',\n",
        "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
        "        'max_bin': trial.suggest_int('max_bin', 200, 400),\n",
        "        #'rsm': trial.suggest_uniform('rsm', 0.3, 1.0),\n",
        "        'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
        "        'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
        "        'n_estimators':  25000,\n",
        "        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15]),\n",
        "        'random_state': trial.suggest_categorical('random_state', [2020]),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n",
        "    }\n",
        "    model = CatBoostRegressor(**param)  \n",
        "    \n",
        "    model.fit(train_inputs,train_targets,eval_set=[(val_inputs,val_targets)],early_stopping_rounds=200,verbose=False)\n",
        "    \n",
        "    preds = model.predict(val_inputs)\n",
        "    \n",
        "    rmse = mean_squared_error(val_targets, preds,squared=False)\n",
        "    \n",
        "    return rmse"
      ],
      "metadata": {
        "id": "BIZWI6MYc8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=30)"
      ],
      "metadata": {
        "id": "tlADEHzfdCL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\n",
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "obG2sP6zdES7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "Totally we trained 6 models\n",
        "* Model 1 - LINEAR REGRESSION MODEL\n",
        "* Model 2 - DECISION TREE MODEL\n",
        "* Model 3 - RANDOM FOREST MODEL\n",
        "* Model 4 - GRADIENT BOOSTING MACHINES MODEL\n",
        "* Model 5 - LIGHTGBM MODEL\n",
        "* Model 6 - CATBOOST MODEL"
      ],
      "metadata": {
        "id": "JViwscBquno0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhYqFtQ1up6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}