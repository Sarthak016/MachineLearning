{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarthak016/MachineLearning/blob/main/Classification_BLUEPRINT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic using Classification\n",
        "\n",
        "The following topics are covered in this colab :\n",
        "\n",
        "- Downloading a real-world dataset\n",
        "- Preparing a dataset for training\n",
        "- Training and interpreting decision trees\n",
        "- Training and interpreting random forests\n",
        "- Overfitting, hyperparameter tuning & regularization\n",
        "- Making predictions on single inputs\n"
      ],
      "metadata": {
        "id": "7Vxy3OgU9Tgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "This tutorial takes a practical and coding-focused approach. We'll define the terms _machine learning_ and _linear regression_ in the context of a problem, and later generalize their definitions. We'll work through a typical machine learning problem step-by-step:\n",
        "\n",
        "\n",
        "> **QUESTION**: ACME Insurance Inc. offers affordable health insurance to thousands of customer all over the United States. As the lead data scientist at ACME, **you're tasked with creating an automated system to estimate the annual medical expenditure for new customers**, using information such as their age, sex, BMI, children, smoking habits and region of residence. \n",
        ">\n",
        "> Estimates from your system will be used to determine the annual insurance premium (amount paid every month) offered to the customer. Due to regulatory requirements, you must be able to explain why your system outputs a certain prediction."
      ],
      "metadata": {
        "id": "sG6-L1a49ZcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1 - Download and Explore the Data\n",
        "\n",
        "The dataset is available as a ZIP file at the following url:"
      ],
      "metadata": {
        "id": "ojBFx6ti9lbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Load the data from the file `train.csv` into a Pandas data frame."
      ],
      "metadata": {
        "id": "tqxkSq_-xaYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas and numpy to read csv file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option(\"max_columns\",None)"
      ],
      "metadata": {
        "id": "Qu-eygJ3jpdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the csv file\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/data.csv\")"
      ],
      "metadata": {
        "id": "9OP3PQR9xiyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "mHG2aEk1-Y1l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "dfda090b-fbe8-4b63-f857-4a409ea10243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
              "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
              "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
              "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
              "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
              "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
              "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
              "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
              "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
              "\n",
              "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "count       569.000000        569.000000      569.000000           569.000000   \n",
              "mean          0.096360          0.104341        0.088799             0.048919   \n",
              "std           0.014064          0.052813        0.079720             0.038803   \n",
              "min           0.052630          0.019380        0.000000             0.000000   \n",
              "25%           0.086370          0.064920        0.029560             0.020310   \n",
              "50%           0.095870          0.092630        0.061540             0.033500   \n",
              "75%           0.105300          0.130400        0.130700             0.074000   \n",
              "max           0.163400          0.345400        0.426800             0.201200   \n",
              "\n",
              "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
              "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
              "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
              "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
              "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
              "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
              "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
              "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
              "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
              "\n",
              "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "count        569.000000         569.000000       569.000000   \n",
              "mean           0.132369           0.254265         0.272188   \n",
              "std            0.022832           0.157336         0.208624   \n",
              "min            0.071170           0.027290         0.000000   \n",
              "25%            0.116600           0.147200         0.114500   \n",
              "50%            0.131300           0.211900         0.226700   \n",
              "75%            0.146000           0.339100         0.382900   \n",
              "max            0.222600           1.058000         1.252000   \n",
              "\n",
              "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
              "count            569.000000      569.000000               569.000000   \n",
              "mean               0.114606        0.290076                 0.083946   \n",
              "std                0.065732        0.061867                 0.018061   \n",
              "min                0.000000        0.156500                 0.055040   \n",
              "25%                0.064930        0.250400                 0.071460   \n",
              "50%                0.099930        0.282200                 0.080040   \n",
              "75%                0.161400        0.317900                 0.092080   \n",
              "max                0.291000        0.663800                 0.207500   \n",
              "\n",
              "       Unnamed: 32  \n",
              "count          0.0  \n",
              "mean           NaN  \n",
              "std            NaN  \n",
              "min            NaN  \n",
              "25%            NaN  \n",
              "50%            NaN  \n",
              "75%            NaN  \n",
              "max            NaN  \n",
              "\n",
              "[8 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95cd7c95-9230-41c8-b3f3-ecdcd8bbc82e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.690000e+02</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.037183e+07</td>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>...</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.250206e+08</td>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>...</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.670000e+03</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>...</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.692180e+05</td>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>...</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.060240e+05</td>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>...</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.813129e+06</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>...</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.113205e+08</td>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>...</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95cd7c95-9230-41c8-b3f3-ecdcd8bbc82e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95cd7c95-9230-41c8-b3f3-ecdcd8bbc82e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95cd7c95-9230-41c8-b3f3-ecdcd8bbc82e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 1338 rows and 7 columns. Each row of the dataset contains information about one customer. \n",
        "\n",
        "Our objective is to find a way to estimate the value in the \"charges\" column using the values in the other columns. If we can do so for the historical data, then we should able to estimate charges for new customers too, simply by asking for information like their age, sex, BMI, no. of children, smoking habits and region.\n",
        "\n",
        "Let's check the data type for each column."
      ],
      "metadata": {
        "id": "yJ5tgOSxxtTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "K9_jXt1F-f91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Here are some statistics for the numerical columns:"
      ],
      "metadata": {
        "id": "rpBh3IpiQlfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "7-TTdcspQpCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">  How many rows and columns does the dataset contain? "
      ],
      "metadata": {
        "id": "C0DMkjx5x0rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_rows = data.shape[0]"
      ],
      "metadata": {
        "id": "pOnPw7cdx4-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_cols = data.shape[1]"
      ],
      "metadata": {
        "id": "rRz3sqzSx4yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The dataset contains {} rows and {} columns.'.format(n_rows, n_cols))"
      ],
      "metadata": {
        "id": "su7es1Vpx4mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Analysis and Visualization\n",
        "\n",
        "Let's explore the data by visualizing the distribution of values in some columns of the dataset, and the relationships between \"charges\" and other columns.\n"
      ],
      "metadata": {
        "id": "Agbs6-ljyGgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* libraries that we are going to use in this collab "
      ],
      "metadata": {
        "id": "cNIR7Kk0zHjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries that we are going to use in this collab \n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "#The following settings will improve the default style and font sizes for our charts\n",
        "sns.set_style('darkgrid')\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
        "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
      ],
      "metadata": {
        "id": "N2J8If1ozGR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> How many `missing values` does the dataset contain in percentage? "
      ],
      "metadata": {
        "id": "n0UAQgDZyQRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 -step make the list of features which has missing values\n",
        "feature_with_na=[feature for feature in data.columns if data[feature].isnull().sum()>1]\n",
        "# 2- step print the feature name and the percentage of missing values\n",
        "for feature in feature_with_na:\n",
        "  print(feature, np.round(data[feature].isnull().mean(), 4)*100,  \" % missing values\")"
      ],
      "metadata": {
        "id": "14IzWrXlFLKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets drop columns which have nan value above 40%\n",
        "perc=40.0\n",
        "min_count=int(((100-perc)/100)*data.shape[0] + 1)\n",
        "data=data.dropna(axis=1,thresh=min_count)"
      ],
      "metadata": {
        "id": "qTo0P04LJWIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U dataprep"
      ],
      "metadata": {
        "id": "bzI-wyasQ0iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using dataprep profiling to get a idea about dataset \n",
        "from dataprep.eda import create_report\n",
        "report=create_report(data)\n",
        "report"
      ],
      "metadata": {
        "id": "pxjQIVkhQy5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Numerical Variables"
      ],
      "metadata": {
        "id": "98KQ9b4UQ5A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of numerical variables\n",
        "numerical_features = [feature for feature in data.columns if data[feature].dtypes != 'O']\n",
        "\n",
        "print('Number of numerical variables: ', len(numerical_features))\n",
        "# visualise the numerical variables\n",
        "data[numerical_features].head()"
      ],
      "metadata": {
        "id": "4AE3JiaJQ7-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets analyse the continuous values by creating histograms to understand the distribution\n",
        "df = data[numerical_features]\n",
        "fig = plt.figure(figsize = (25, 35))\n",
        "i=1\n",
        "for n in df.columns:\n",
        "    plt.subplot(7, 5, i)\n",
        "    figure = sns.histplot(x = data[n],hue = data['diagnosis'], palette = ['#676FA3', '#FF5959'], bins = 40)\n",
        "    figure.set(xlabel = None, ylabel = None)\n",
        "    plt.title(str(n), loc = 'center')\n",
        "    plt.xticks(rotation = 20, fontsize = 10)\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "8mpbFdfaGw-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Categorical Variables"
      ],
      "metadata": {
        "id": "uDPaFot3RKV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of categorical variables\n",
        "categorical_features = [feature for feature in data.columns if data[feature].dtypes == 'O']\n",
        "\n",
        "print('Number of categorical variables: ', len(categorical_features))\n",
        "# visualise the categorical variables\n",
        "data[categorical_features].head()"
      ],
      "metadata": {
        "id": "bEfl_3w6RNbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique number of categorical features\n",
        "for feature in categorical_features:\n",
        "    print('The feature is {} and number of categories are {}'.format(feature,len(data[feature].unique())))"
      ],
      "metadata": {
        "id": "CWooUMOFRXda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find out the relationship between categorical variable and dependent feature\n",
        "\n",
        "df = data[categorical_features]\n",
        "plt.figure(figsize = (25, 25))\n",
        "i = 1\n",
        "for c in df.columns:\n",
        "    plt.subplot(5, 2, i)\n",
        "    figure = sns.countplot(data = data, x = data[c], hue = 'TARGET', palette = ['#676FA3', '#FF5959'])\n",
        "    figure.set(xlabel = None, ylabel = None)\n",
        "    plt.title(str(c), loc='center')\n",
        "    plt.xticks( fontsize = 10)\n",
        "    i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne7loekKdPz6",
        "outputId": "b61bb3b2-511c-4d58-cf05-45e5d7c4ab1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete Variables Count: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Outliers"
      ],
      "metadata": {
        "id": "09M_uFLAR3x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in numerical_features:\n",
        "    dataset=data.copy()\n",
        "    if 0 in dataset[feature].unique():\n",
        "        pass\n",
        "    else:\n",
        "        dataset[feature]=np.log(dataset[feature])\n",
        "        dataset.boxplot(column=feature)\n",
        "        plt.ylabel(feature)\n",
        "        plt.title(feature)\n",
        "        plt.show()   "
      ],
      "metadata": {
        "id": "0vDzvdfTR1yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation"
      ],
      "metadata": {
        "id": "hQBTIvZ4SABL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(abs(data.corr()),annot=True,cmap='coolwarm',linewidth=1,linecolor='black')"
      ],
      "metadata": {
        "id": "kU36diKLSCuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Prepare the Dataset for Training\n",
        "\n",
        "\n",
        "Before we can train the model, we need to prepare the dataset. Here are the steps we'll follow:\n",
        "\n",
        "1. Identify the input and target column(s) for training the model.\n",
        "2. Identify numeric and categorical input columns.\n",
        "3. [Impute](https://scikit-learn.org/stable/modules/impute.html) (fill) missing values in numeric columns\n",
        "4. [Scale](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a $(0,1)$ range.\n",
        "5. [Encode](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) categorical data into one-hot vectors.\n",
        "6. Split the dataset into training and validation sets.\n"
      ],
      "metadata": {
        "id": "0WwkH6uC9_EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify Inputs and Targets\n",
        "\n",
        "While the dataset contains `81` columns, not all of them are useful for modeling. Note the following:\n",
        "\n",
        "- The first column `Id` is a unique ID for each house and isn't useful for training the model.\n",
        "- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.\n",
        "- Data from all the other columns (except the first and the last column) can be used as inputs to the model.\n",
        " "
      ],
      "metadata": {
        "id": "s2qnXMj_h2z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Create a list `input_cols` of column names containing data that can be used as input to train the model, and identify the target column as the variable `target_col`."
      ],
      "metadata": {
        "id": "zvWhINVt0egw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the input columns (a list of column names)\n",
        "input_cols = list(data.columns)[1:-1]\n",
        "\n",
        "# Identify the name of the target column (a single string, not a list)\n",
        "target_col =list(data.columns)[-1]"
      ],
      "metadata": {
        "id": "7T7AeTcxhvzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice whatever code u execute, print and check it \n",
        "print(input_cols)"
      ],
      "metadata": {
        "id": "fc-DatjX04qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice whatever code u execute, print and check it \n",
        "print(target_col)"
      ],
      "metadata": {
        "id": "7HHkRWNm03dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure that the `Id` and `SalePrice` columns are not included in `input_cols`.\n",
        "\n",
        "Now that we've identified the input and target columns, we can separate input & target data."
      ],
      "metadata": {
        "id": "JsdUeloo0wJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate input & target data\n",
        "inputs_df = data[input_cols]\n",
        "targets = data[target_col]"
      ],
      "metadata": {
        "id": "TQJmCsQo095e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Identify Numeric and Categorical Data\n",
        "The next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column."
      ],
      "metadata": {
        "id": "EwazBRPDiNap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 5**: Crate two lists `numeric_cols` and `categorical_cols` containing names of numeric and categorical input columns within the dataframe respectively. Numeric columns have data types `int64` and `float64`, whereas categorical columns have the data type `object`.\n",
        ">\n",
        "> *Hint*: See this [StackOverflow question](https://stackoverflow.com/questions/25039626/how-do-i-find-numeric-columns-in-pandas). "
      ],
      "metadata": {
        "id": "d9ocavp91cdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#numerical=medical.select_dtypes(include=np.number).columns.tolist()\n",
        "numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = inputs_df.select_dtypes(include=[object]).columns.tolist()"
      ],
      "metadata": {
        "id": "Vr6XCGsPiNAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Impute Numerical Data\n",
        "Some of the numeric columns in our dataset contain missing values (nan)"
      ],
      "metadata": {
        "id": "F5XbLVc2ic2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using isna() to calculate the null values in Numeric columns\n",
        "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\n",
        "missing_counts[missing_counts > 0]"
      ],
      "metadata": {
        "id": "l3FsYhw4iecs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning models can't work with missing data. The process of filling missing values is called [imputation](https://scikit-learn.org/stable/modules/impute.html).\n",
        "\n",
        "<img src=\"https://i.imgur.com/W7cfyOp.png\" width=\"480\">\n",
        "\n",
        "There are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`.\n"
      ],
      "metadata": {
        "id": "OfIDO-cv1uB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 6**: Impute (fill) missing values in the numeric columns of `inputs_df` using a `SimpleImputer`. "
      ],
      "metadata": {
        "id": "91YfP3zG18Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SimpleImputer from sklearn library\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 1. Create the imputer\n",
        "imputer = SimpleImputer(strategy = 'mean')\n",
        "\n",
        "# 2. Fit the imputer to the numeric colums\n",
        "imputer.fit(inputs_df[numeric_cols])\n",
        "\n",
        "# 3. Transform and replace the numeric columns\n",
        "inputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])"
      ],
      "metadata": {
        "id": "NUGBkrYH1p8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using isna()  to check the null values in Numeric columns\n",
        "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\n",
        "missing_counts[missing_counts > 0]"
      ],
      "metadata": {
        "id": "bEIEeYBH1_Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scale Numerical Values\n",
        "The numeric columns in our dataset have varying ranges."
      ],
      "metadata": {
        "id": "K0xV7nPui-VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using describe function to see statistics information and .loc to filter min and max from describe function\n",
        "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
      ],
      "metadata": {
        "id": "KkHP1jBNjCoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good practice is to [scale numeric features](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) to a small range of values e.g. $(0,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.\n"
      ],
      "metadata": {
        "id": "RfHBHiPI2JU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 7**: Scale numeric values to the $(0, 1)$ range using `MinMaxScaler` from `sklearn.preprocessing`."
      ],
      "metadata": {
        "id": "SEZvAGJt2NRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MinMaxScaler from sklearn library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the numeric columns\n",
        "scaler.fit(inputs_df[numeric_cols])\n",
        "\n",
        "# Transform and replace the numeric columns\n",
        "inputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])"
      ],
      "metadata": {
        "id": "xRcn3CHs2F8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After scaling, the ranges of all numeric columns should be (0, 1)."
      ],
      "metadata": {
        "id": "o-l3QJBm2ccj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check that scaling worked or not\n",
        "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
      ],
      "metadata": {
        "id": "UioP8mNB2gq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encode Categorical Columns\n",
        "Our dataset contains several categorical columns, each with a different number of categories."
      ],
      "metadata": {
        "id": "tl5Hio73jG0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing unique Categorical columns \n",
        "inputs_df[categorical_cols].nunique().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "osf6IG2mjIgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.\n",
        "\n",
        "<img src=\"https://i.imgur.com/n8GuiOO.png\" width=\"640\">\n",
        "\n",
        "One hot encoding involves adding a new binary (0/1) column for each unique category of a categorical column."
      ],
      "metadata": {
        "id": "K-lHn31D2riG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 8**: Encode categorical columns in the dataset as one-hot vectors using `OneHotEncoder` from `sklearn.preprocessing`. Add a new binary (0/1) column for each category"
      ],
      "metadata": {
        "id": "crFnxAFD2zHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import OneHotEncoder from sklearn library\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 1. Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "\n",
        "# 2. Fit the encoder to the categorical colums\n",
        "encoder.fit(inputs_df[categorical_cols])\n",
        "\n",
        "# 3. Generate column names for each category\n",
        "encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
        "len(encoded_cols)"
      ],
      "metadata": {
        "id": "FZawE_Yx28g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Transform and add new one-hot category columns\n",
        "inputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])"
      ],
      "metadata": {
        "id": "UeiagrpW3BNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new one-hot category columns should now be added to `inputs_df`."
      ],
      "metadata": {
        "id": "2mnsgZ1o3F1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Validation Set\n",
        "Finally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers."
      ],
      "metadata": {
        "id": "uVS7HjBzjKAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import train_test_split from sklearn library to make split of data into train sets and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df[numeric_cols + encoded_cols], \n",
        "                                                                        targets, \n",
        "                                                                        test_size=0.25, \n",
        "                                                                        random_state=42)\n"
      ],
      "metadata": {
        "id": "Fz1MG0IFjPWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "train_inputs"
      ],
      "metadata": {
        "id": "GHJr-XzE3P47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "train_targets"
      ],
      "metadata": {
        "id": "Beb7sLS83SiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "val_inputs"
      ],
      "metadata": {
        "id": "UAVwiPms3W-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It always a good practice to print and check the executed codes.\n",
        "val_targets"
      ],
      "metadata": {
        "id": "TTPqItJq3Yyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "xm4nL22SfBR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "id": "Ie3lTFq8fH0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "import lightgbm \n"
      ],
      "metadata": {
        "id": "9elMulyBfGB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "           ['LogisticRegression: ',              LogisticRegression()],\n",
        "           ['KNeighborsClassifier: ',            KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')],\n",
        "           ['SVC: ',                             SVC(kernel='linear', C=1.0, random_state=0)],\n",
        "           ['DecisionTreeClassifier: ',          DecisionTreeClassifier(random_state=42)],\n",
        "           ['RandomForestClassifier:' ,          RandomForestClassifier(random_state=42)],\n",
        "           ['ExtraTreesClassifier ',             ExtraTreesClassifier(random_state=42)],\n",
        "           ['GradientBoostingClassifier ',       GradientBoostingClassifier(random_state=42)],\n",
        "           ['XGBClassifier :',                   XGBClassifier(objective= 'binary:logistic',random_state=42)],\n",
        "           ['Light-GBM: ',                     lightgbm.LGBMRegressor(random_state=42)]\n",
        "          ]\n",
        "           \n",
        "           "
      ],
      "metadata": {
        "id": "TTSUfKwpfJ4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all the proposed models and update the information in a list model_data\n",
        "import time\n",
        "from math import sqrt\n",
        "from sklearn import metrics\n",
        "\n",
        "model_data = []\n",
        "for name,model in models :\n",
        "\n",
        "    model_data_dic = {}\n",
        "    model_data_dic[\"Name\"] = name\n",
        "    start = time.time()\n",
        "    end = time.time()\n",
        "    model.fit(train_inputs,train_targets) \n",
        "    model_data_dic[\"Train_Time\"] = end - start\n",
        "    # Training set\n",
        "    model_data_dic[\"Train_Accuracy\"] = metrics.accuracy_score(train_targets, model.predict(train_inputs))\n",
        "    # Validation set\n",
        "    model_data_dic[\"Test_Accuracy\"] = metrics.accuracy_score(val_targets, model.predict(val_inputs))\n",
        "\n",
        "    model_data.append(model_data_dic)"
      ],
      "metadata": {
        "id": "kmqKArV3fL3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list to dataframe\n",
        "df = pd.DataFrame(model_data)"
      ],
      "metadata": {
        "id": "XkJudmlObPYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot(x=\"Name\", y=['Test_R2_Score' , 'Train_R2_Score' , 'Test_RMSE_Score'], kind=\"bar\" , title = 'R2 Score Results' , figsize= (10,8)) ;"
      ],
      "metadata": {
        "id": "QXAllmSyfSUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Obervations\n",
        "1. Best results over test set are given by Extra Tree Regressor with R2 score of 0.57\n",
        "2. Least RMSE score is also by Extra Tree Regressor 0.65\n",
        "3. Lasso regularization over Linear regression was worst performing model"
      ],
      "metadata": {
        "id": "94sxJCTZfWHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "# generate evaluation metrics for training set\n",
        "def evalaute_train(model,train_inputs,train_targets):\n",
        "  \n",
        "    print (\"Train - Accuracy :\", metrics.accuracy_score(train_targets, model.predict(train_inputs)))\n",
        "    print (\"Train - AUC :\", metrics.roc_auc_score(train_targets, model.predict_proba(train_inputs)[:,1]))\n",
        "    print (\"Train - Confusion matrix :\",metrics.confusion_matrix(train_targets, model.predict(train_inputs)))\n",
        "    print (\"-----------------------------------------------------------------------------------------\")\n",
        "    print (\"Train - classification report :\", metrics.classification_report(train_targets, model.predict(train_inputs)))\n",
        "\n",
        "# generate evaluation metrics for training set\n",
        "def evalaute_test(model,val_inputs,val_targets):\n",
        "\n",
        "    print (\"Test - Accuracy :\", metrics.accuracy_score(val_targets, model.predict(val_inputs)))\n",
        "    print (\"Test - AUC :\", metrics.roc_auc_score(val_targets, model.predict_proba(val_inputs)[:,1]))\n",
        "    print (\"Test - Confusion matrix :\",metrics.confusion_matrix(val_targets, model.predict(val_inputs)))\n",
        "    print (\"-----------------------------------------------------------------------------------------\")\n",
        "    print (\"Test - classification report :\", metrics.classification_report(val_targets, model.predict(val_inputs)))\n"
      ],
      "metadata": {
        "id": "tCBaqHnXj6RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 - Train a Logistic Regression Model\n",
        "\n",
        "\n",
        "Logistic regression is a commonly used technique for solving binary classification problems. In a logistic regression model: \n",
        "\n",
        "- we take linear combination (or weighted sum of the input features) \n",
        "- we apply the sigmoid function to the result to obtain a number between 0 and 1\n",
        "- this number represents the probability of the input being classified as \"Yes\"\n",
        "- instead of RMSE, the cross entropy loss function is used to evaluate the results\n",
        "\n",
        "\n",
        "Here's a visual summary of how a logistic regression model is structured ([source](http://datahacker.rs/005-pytorch-logistic-regression-in-pytorch/)):\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/YMaMo5D.png\" width=\"480\">\n",
        "\n",
        "The sigmoid function applied to the linear combination of inputs has the following formula:\n",
        "\n",
        "<img src=\"https://i.imgur.com/sAVwvZP.png\" width=\"400\">\n",
        "\n",
        "\n",
        "The output of the sigmoid function is called a logistic, hence the name _logistic regression_. For a mathematical discussion of logistic regression, sigmoid activation and cross entropy, check out [this YouTube playlist](https://www.youtube.com/watch?v=-la3q9d7AKQ&list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy&index=1). Logistic regression can also be applied to multi-class classification problems, with a few modifications.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IY7dqLjyk4ZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 9**: Create and train a linear regression model using the `Ridge` class from `sklearn.linear_model`."
      ],
      "metadata": {
        "id": "KW5zJVpP3myr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate a logistic regression model, and fit with train_inputs and train_targets\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "cO4lE9kDk6HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model.fit` uses the following strategy for training the model (source):\n",
        "\n",
        "1. We initialize a model with random parameters (weights & biases).\n",
        "2. We pass some inputs into the model to obtain predictions.\n",
        "3. We compare the model's predictions with the actual targets using the loss function.\n",
        "4. We use an optimization technique (like least squares, gradient descent etc.) to reduce the loss by adjusting the weights & biases of the model\n",
        "5. We repeat steps 1 to 4 till the predictions from the model are good enough.\n",
        "\n",
        "<img src=\"https://www.deepnetts.com/blog/wp-content/uploads/2019/02/SupervisedLearning.png\" width=\"480\">"
      ],
      "metadata": {
        "id": "qJx5kKiOlCTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_train(model,train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "chC6YmAAjJbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_test(model,val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "9WEMo0sD4RC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regularization\n",
        "With an increase in the number of variables, the probability of over-fitting also increases.\n",
        "`LASSO (L1)` and `Ridge (L2)` can be applied for logistic regression as well to avoid overfitting."
      ],
      "metadata": {
        "id": "Z6rVzhPCmUNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# l1 regularization gives better results\n",
        "\n",
        "model = LogisticRegression(penalty='l1', C=10, random_state=0)\n",
        "\n",
        "model.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "rnocMZ7dmTwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_train(model,train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "5gat1ZIEn0rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_test(model,val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "AJHKbOHRn2Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance\n",
        "\n",
        "Let's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important."
      ],
      "metadata": {
        "id": "d89v9NyplSKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **QUESTION 11**: Identify the weights (or coefficients) assigned to for different features by the model.\n",
        "> \n",
        "> *Hint:* Read [the docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
      ],
      "metadata": {
        "id": "FjTOj_Oy4bdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.coef_.flatten()"
      ],
      "metadata": {
        "id": "-gH7_W7tlTOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a dataframe to view the weight assigned to each column."
      ],
      "metadata": {
        "id": "u4xCHhPDlYcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_df = pd.DataFrame({\n",
        "    'columns': train_inputs.columns,\n",
        "    'weight': weights\n",
        "}).sort_values('weight', ascending=False)"
      ],
      "metadata": {
        "id": "5qOJx4tPlavS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Feature Importance')\n",
        "sns.barplot(data=weights_df.head(10), x='weight', y='columns');"
      ],
      "metadata": {
        "id": "g6NZqbOeld_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 - Train a Support Vector Machine (SVM)"
      ],
      "metadata": {
        "id": "9_zqDXP6vLH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  SVM is comparatively less prone to outliers than logistic regression as it only\n",
        "cares about the points that are closest to the decision boundary or support vectors.\n",
        "\n",
        "\n",
        "2. Key Parameters\n",
        "* C: This is the penalty parameter and helps in fitting the boundaries smoothly and\n",
        "appropriately, default=1\n",
        "* Kernel: A kernel is a similarity function for pattern analysis. It must be one of rbf/\n",
        "linear/poly/sigmoid/precomputed, default=’rbf’ (Radial Basis Function). Choosing an\n",
        "appropriate kernel will result in a better model fit"
      ],
      "metadata": {
        "id": "7tK2XPmFvlrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='linear', C=1.0, random_state=0)\n",
        "model.fit(train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "jysRMnyrvKy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_train(model,train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "qN_A9Qabw5lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_test(model,val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "z2Xqw3oUw8bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ploting SVM decision boundaries"
      ],
      "metadata": {
        "id": "Xxp82JVkxKZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use sklearn make_classification function to create some test data.\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(100, 2, 2, 0, weights=[.5, .5], random_state=0)\n",
        "# build a simple logistic regression model\n",
        "clf = SVC(kernel='linear', random_state=0)\n",
        "clf.fit(X, y)\n",
        "# get the separating hyperplane\n",
        "w = clf.coef_[0]\n",
        "a = -w[0] / w[1]\n",
        "xx = np.linspace(-5, 5)\n",
        "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
        "# plot the parallels to the separating hyperplane that pass through the\n",
        "# support vectors\n",
        "b = clf.support_vectors_[0]\n",
        "yy_down = a * xx + (b[1] - a * b[0])\n",
        "b = clf.support_vectors_[-1]\n",
        "yy_up = a * xx + (b[1] - a * b[0])\n",
        "\n",
        "# Plot the decision boundary\n",
        "plot_decision_regions(X, y, classifier=clf)\n",
        "# plot the line, the points, and the nearest vectors to the plane\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
        "facecolors='none')\n",
        "plt.plot(xx, yy_down, 'k--')\n",
        "plt.plot(xx, yy_up, 'k--')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BsszEpm2xHkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3 - Train a k-Nearest Neighbors (kNN)"
      ],
      "metadata": {
        "id": "qC6AF3D9xsKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
        "model.fit(train_inputs,train_targets)\n"
      ],
      "metadata": {
        "id": "5PyVOABDx6RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_train(model,train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "Ctr6MUzOyWgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_test(model,val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "g7JEQsNAybzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4 -Training and Visualizing Decision Trees\n",
        "\n",
        "A decision tree in general parlance represents a hierarchical series of binary decisions:\n",
        "\n",
        "<img src=\"https://i.imgur.com/qSH4lqz.png\" width=\"480\">\n",
        "\n",
        "A decision tree in machine learning works in exactly the same way, and except that we let the computer figure out the optimal structure & hierarchy of decisions, instead of coming up with criteria manually."
      ],
      "metadata": {
        "id": "p0jz6w6hfo_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "We can use `DecisionTreeClassifier` from `sklearn.tree` to train a decision tree."
      ],
      "metadata": {
        "id": "vqGbbfjPf9Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create the model\n",
        "model = DecisionTreeClassifier(criterion = 'entropy',random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, train_targets)"
      ],
      "metadata": {
        "id": "kO40lLfifzUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimal decision tree has now been created using the training data."
      ],
      "metadata": {
        "id": "SnYtlWkxhsry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n",
        "\n",
        "Let's evaluate the decision tree using the accuracy score."
      ],
      "metadata": {
        "id": "qvnCslvqgXhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_train(model,train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "RB76kWs0f7CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_test(model,val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "7h4uS5dHg7aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree also returns probabilities for each prediction."
      ],
      "metadata": {
        "id": "xhtOEM7ghQxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set accuracy is close to 100%! But we can't rely solely on the training set accuracy, we must evaluate the model on the validation set too. \n",
        "\n",
        "We can make predictions and compute accuracy in one step using `model.score`"
      ],
      "metadata": {
        "id": "Y9umuDGMhPz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the training accuracy is 100%, the accuracy on the validation set is just about 79%, which is only marginally better then always predicting \"No\". "
      ],
      "metadata": {
        "id": "GgxSvGVYiMr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "\n",
        "We can visualize the decision tree _learned_ from the training data."
      ],
      "metadata": {
        "id": "4vkp9yYViZrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree, export_text\n",
        "plt.figure(figsize=(80,20))\n",
        "plot_tree(model, feature_names=X_train.columns, max_depth=2, filled=True);"
      ],
      "metadata": {
        "id": "u_ZvV61viXBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you see how the model classifies a given input as a series of decisions? The tree is truncated here, but following any path from the root node down to a leaf will result in \"Yes\" or \"No\". Do you see how a decision tree differs from a logistic regression model?\n",
        "\n",
        "\n",
        "**How a Decision Tree is Created**\n",
        "\n",
        "Note the `gini` value in each box. This is the loss function used by the decision tree to decide which column should be used for splitting the data, and at what point the column should be split. A lower Gini index indicates a better split. A perfect split (only one class on each side) has a Gini index of 0. \n",
        "\n",
        "For a mathematical discussion of the Gini Index, watch this video: https://www.youtube.com/watch?v=-W0DnxQK1Eo . It has the following formula:\n",
        "\n",
        "<img src=\"https://i.imgur.com/CSC0gAo.png\" width=\"240\">\n",
        "\n",
        "Conceptually speaking, while training the models evaluates all possible splits across all possible columns and picks the best one. Then, it recursively performs an optimal split for the two portions. In practice, however, it's very inefficient to check all possible splits, so the model uses a heuristic (predefined strategy) combined with some randomization.\n",
        "\n",
        "The iterative approach of the machine learning workflow in the case of a decision tree involves growing the tree layer-by-layer:\n",
        "\n",
        "<img src=\"https://www.deepnetts.com/blog/wp-content/uploads/2019/02/SupervisedLearning.png\" width=\"480\">\n",
        "\n",
        "\n",
        "Let's check the depth of the tree that was created."
      ],
      "metadata": {
        "id": "1Vznv8keikyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.tree_.max_depth"
      ],
      "metadata": {
        "id": "H9v0BIjpilfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also display the tree as text, which can be easier to follow for deeper trees."
      ],
      "metadata": {
        "id": "csuRXS-vjFhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_text = export_text(model, max_depth=10, feature_names=list(X_train.columns))\n",
        "print(tree_text[:5000])"
      ],
      "metadata": {
        "id": "5sJrkR6GjIP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance\n",
        "\n",
        "Based on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree."
      ],
      "metadata": {
        "id": "89UALm00jMk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.feature_importances_"
      ],
      "metadata": {
        "id": "bMmZzrwGjNlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's turn this into a dataframe and visualize the most important features."
      ],
      "metadata": {
        "id": "nJqarmmLjW2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)"
      ],
      "metadata": {
        "id": "x5BObc-6jULf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head(10)"
      ],
      "metadata": {
        "id": "1Fges4NFjaBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Feature Importance')\n",
        "sns.barplot(data=importance_df.head(10), x='importance', y='feature');"
      ],
      "metadata": {
        "id": "PWmpT3PjjcBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning and Overfitting\n",
        "\n",
        "As we saw in the previous section, our decision tree classifier memorized all training examples, leading to a 100% training accuracy, while the validation accuracy was only marginally better than a dumb baseline model. This phenomenon is called overfitting, and in this section, we'll look at some strategies for reducing overfitting. The process of reducing overfitting is known as _regularlization_.\n",
        "\n",
        "\n",
        "The `DecisionTreeClassifier` accepts several arguments, some of which can be modified to reduce overfitting."
      ],
      "metadata": {
        "id": "7c84N6kIjjIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These arguments are called hyperparameters because they must be configured manually (as opposed to the parameters within the model which are _learned_ from the data. We'll explore a couple of hyperparameters:\n",
        "\n",
        "- `max_depth`\n",
        "- `max_leaf_nodes`"
      ],
      "metadata": {
        "id": "RQEn9u4ZjxpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `max_depth`\n",
        "\n",
        "By reducing the maximum depth of the decision tree, we can prevent the tree from memorizing all training examples, which may lead to better generalization"
      ],
      "metadata": {
        "id": "6mAQL-Kxj1iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "model.fit(X_train, train_targets)"
      ],
      "metadata": {
        "id": "dixX08rDjeOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute the accuracy of the model on the training and validation sets using `model.score`"
      ],
      "metadata": {
        "id": "tA38pBd0j9nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_train, train_targets)"
      ],
      "metadata": {
        "id": "uytcGGapkBL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_val, val_targets)"
      ],
      "metadata": {
        "id": "Y8mqmW9akB9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, while the training accuracy of the model has gone down, the validation accuracy of the model has increased significantly."
      ],
      "metadata": {
        "id": "XT7cFWnNkHrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.classes_"
      ],
      "metadata": {
        "id": "JZx-9Cj-kpvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(80,20))\n",
        "plot_tree(model, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);"
      ],
      "metadata": {
        "id": "tofEHMNGkDuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(export_text(model, feature_names=list(X_train.columns)))"
      ],
      "metadata": {
        "id": "wMuKef1EkuL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's experiment with different depths using a helper function."
      ],
      "metadata": {
        "id": "3fvBS5UFkyl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_params(**params):\n",
        "    model = DecisionTreeClassifier(random_state=42,**params).fit(train_inputs, train_targets)\n",
        "    Train_score = accuracy_score(model.predict(train_inputs), train_targets)\n",
        "    Val_score = accuracy_score(model.predict(val_inputs), val_targets)\n",
        "    return Train_score, Val_score\n",
        "accuracy_score(train_targets, train_preds)\n",
        "\n",
        "def test_param_and_plot(param_name, param_values):\n",
        "    train_errors, val_errors = [], [] \n",
        "    for value in param_values:\n",
        "        params = {param_name: value}\n",
        "        train_rmse, val_rmse = test_params(**params)\n",
        "        train_errors.append(train_rmse)\n",
        "        val_errors.append(val_rmse)\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.title('Overfitting curve: ' + param_name)\n",
        "    plt.plot(param_values, train_errors, 'b-o')\n",
        "    plt.plot(param_values, val_errors, 'r-o')\n",
        "    plt.xlabel(param_name)\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend(['Training', 'Validation'])"
      ],
      "metadata": {
        "id": "aVZYFX1OVT37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_depth_error(md):\n",
        "    model = DecisionTreeClassifier(max_depth=md, random_state=42)\n",
        "    model.fit(X_train, train_targets)\n",
        "    train_acc = 1 - model.score(X_train, train_targets)\n",
        "    val_acc = 1 - model.score(X_val, val_targets)\n",
        "    return {'Max Depth': md, 'Training Error': train_acc, 'Validation Error': val_acc}"
      ],
      "metadata": {
        "id": "HEh4IgAYkzQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors_df = pd.DataFrame([max_depth_error(md) for md in range(1, 21)])"
      ],
      "metadata": {
        "id": "5JvT_7ZGk8gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(errors_df['Max Depth'], errors_df['Training Error'])\n",
        "plt.plot(errors_df['Max Depth'], errors_df['Validation Error'])\n",
        "plt.title('Training vs. Validation Error')\n",
        "plt.xticks(range(0,21, 2))\n",
        "plt.xlabel('Max. Depth')\n",
        "plt.ylabel('Prediction Error (1 - Accuracy)')\n",
        "plt.legend(['Training', 'Validation'])"
      ],
      "metadata": {
        "id": "1F2hesEkk_lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a common pattern you'll see with all machine learning algorithms:\n",
        "\n",
        "<img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"480\">\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GB0BiO0qlCZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll often need to tune hyperparameters carefully to find the optimal fit. In the above case, it appears that a maximum depth of 7 results in the lowest validation error."
      ],
      "metadata": {
        "id": "78UcUW-hlGb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreeClassifier(max_depth=7, random_state=42).fit(X_train, train_targets)\n",
        "model.score(X_val, val_targets)"
      ],
      "metadata": {
        "id": "wMCDTgBplFn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `max_leaf_nodes`\n",
        "\n",
        "Another way to control the size of complexity of a decision tree is to limit the number of leaf nodes. This allows branches of the tree to have varying depths. "
      ],
      "metadata": {
        "id": "KljLBRxilMR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreeClassifier(max_leaf_nodes=128, random_state=42)\n",
        "model.fit(X_train, train_targets)"
      ],
      "metadata": {
        "id": "BR1Lj8jdlJZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_train, train_targets)"
      ],
      "metadata": {
        "id": "al4FKRpOlcTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_val, val_targets)"
      ],
      "metadata": {
        "id": "cyNOzGbxlhLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.tree_.max_depth"
      ],
      "metadata": {
        "id": "Hvr2PQWSlpZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notice that the model was able to achieve a greater depth of 12 for certain paths while keeping other paths shorter."
      ],
      "metadata": {
        "id": "8_PseccLlsDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_text = export_text(model, feature_names=list(X_train.columns))\n",
        "print(model_text[:3000])"
      ],
      "metadata": {
        "id": "mjzcLg-vlufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5 -Training a Random Forest\n",
        "\n",
        "While tuning the hyperparameters of a single decision tree may lead to some improvements, a much more effective strategy is to combine the results of several decision trees trained with slightly different parameters. This is called a random forest model. \n",
        "\n",
        "The key idea here is that each decision tree in the forest will make different kinds of errors, and upon averaging, many of their errors will cancel out. This idea is also commonly known as the \"wisdom of the crowd\":\n",
        "\n",
        "<img src=\"https://i.imgur.com/4Dg0XK4.png\" width=\"480\">"
      ],
      "metadata": {
        "id": "hq4HsthJl3KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "num_trees = 100\n",
        "\n",
        "kfold = cross_validation.StratifiedKFold(y=y_train, n_folds=5, random_state=2017)\n",
        "num_trees = 100\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=num_trees)\n",
        "model.fit(train_inputs, train_targets)\n",
        "results = cross_validation.cross_val_score(model, train_inputs, train_targets, cv=kfold)\n",
        "print(\"\\nRandom Forest (Bagging) - Train : \", results.mean())\n",
        "\n",
        "print(\"Random Forest (Bagging) - Test : \", metrics.accuracy_score(model.predict(val_inputs), val_targets))"
      ],
      "metadata": {
        "id": "VYMUCKdX0Uxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`n_jobs` allows the random forest to use mutiple parallel workers to train decision trees, and `random_state=42` ensures that the we get the same results for each execution."
      ],
      "metadata": {
        "id": "FR_O2M3QYJIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_train(model,train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "IMnIG71wlw72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_test(model,val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "iof8JdpIlw28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, the training accuracy is almost 100%, but this time the validation accuracy is much better. In fact, it is better than the best single decision tree we had trained so far. Do you see the power of random forests?\n",
        "\n",
        "This general technique of combining the results of many models is called \"ensembling\", it works because most errors of individual models cancel out on averaging. Here's what it looks like visually:\n",
        "\n",
        "<img src=\"https://i.imgur.com/qJo8D8b.png\" width=\"640\">\n",
        "\n",
        "\n",
        "We can also look at the probabilities for the predictions. The probability of a class is simply the fraction of trees which that predicted the given class."
      ],
      "metadata": {
        "id": "rfpuDHi-YV_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_probs = model.predict_proba(X_train)\n",
        "train_probs"
      ],
      "metadata": {
        "id": "Qqlh--RrYX_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can can access individual decision trees using `model.estimators_`"
      ],
      "metadata": {
        "id": "i2ykDPIVYdFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.estimators_[0]"
      ],
      "metadata": {
        "id": "jQCyoN1yYX5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(80,20))\n",
        "plot_tree(model.estimators_[0], max_depth=2, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);"
      ],
      "metadata": {
        "id": "htgCqukbYX01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(80,20))\n",
        "plot_tree(model.estimators_[20], max_depth=2, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_);"
      ],
      "metadata": {
        "id": "F1obo_yNYXrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.estimators_)"
      ],
      "metadata": {
        "id": "bDf7S75IYXmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance"
      ],
      "metadata": {
        "id": "HkZZqqoMYvRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like decision tree, random forests also assign an \"importance\" to each feature, by combining the importance values from individual trees."
      ],
      "metadata": {
        "id": "MJtij0PNYpIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)"
      ],
      "metadata": {
        "id": "WIHlVX6Olwwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Feature Importance')\n",
        "sns.barplot(data=importance_df.head(10), x='importance', y='feature');"
      ],
      "metadata": {
        "id": "Z0kKGRQdY1Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notice that the distribution is a lot less skewed than that for a single decision tree."
      ],
      "metadata": {
        "id": "7UUkBbJYY6ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning with Random Forests\n",
        "\n",
        "Just like decision trees, random forests also have several hyperparameters. In fact many of these hyperparameters are applied to the underlying decision trees. \n",
        "\n",
        "Let's study some the hyperparameters for random forests. You can learn more about them here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ],
      "metadata": {
        "id": "4eYjf__gY_eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "?RandomForestClassifier"
      ],
      "metadata": {
        "id": "aX8DxeDVY1Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a base model with which we can compare models with tuned hyperparameters."
      ],
      "metadata": {
        "id": "5PLXhA1VZETE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = RandomForestClassifier(random_state=42, n_jobs=-1).fit(X_train, train_targets)"
      ],
      "metadata": {
        "id": "9bHTlC47Y085"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_train_acc = base_model.score(X_train, train_targets)\n",
        "base_val_acc = base_model.score(X_val, val_targets)"
      ],
      "metadata": {
        "id": "0hpjjeFIY035"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs = base_train_acc, base_val_acc\n",
        "base_accs"
      ],
      "metadata": {
        "id": "m6REV6RBY0zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use this as a benchmark for hyperparmeter tuning."
      ],
      "metadata": {
        "id": "qKY6vUnJZQMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `n_estimators`\n",
        "\n",
        "This argument controls the number of decision trees in the random forest. The default value is 100. For larger datasets, it helps to have a greater number of estimators. As a general rule, try to have as few estimators as needed. \n",
        "\n",
        "\n",
        "**10 estimators**"
      ],
      "metadata": {
        "id": "REuGIXeq5ImK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=10)\n",
        "model.fit(X_train, train_targets)"
      ],
      "metadata": {
        "id": "obo0vXdBZRAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_train, train_targets), model.score(X_val, val_targets)"
      ],
      "metadata": {
        "id": "_KTEDAUs5WmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "3te8G5aO5ZlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `max_depth` and `max_leaf_nodes`\n",
        "\n",
        "These arguments are passed directly to each decision tree, and control the maximum depth and max. no leaf nodes of each tree respectively. By default, no maximum depth is specified, which is why each tree has a training accuracy of 100%. You can specify a `max_depth` to reduce overfitting.\n",
        "\n",
        "<img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"480\">\n"
      ],
      "metadata": {
        "id": "yrIs_fir5eQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a helper function test_params to make it easy to test hyperparameters"
      ],
      "metadata": {
        "id": "H0KC8yf75ktS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_params(**params):\n",
        "    model = RandomForestClassifier(random_state=42, n_jobs=-1, **params).fit(X_train, train_targets)\n",
        "    return model.score(X_train, train_targets), model.score(X_val, val_targets)"
      ],
      "metadata": {
        "id": "QUlS9N_K5e5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test a few values of `max_depth` and `max_leaf_nodes`."
      ],
      "metadata": {
        "id": "7RzaU3wM5qY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(max_depth=10,max_leaf_nodes=300)"
      ],
      "metadata": {
        "id": "L1SWWkFq5wZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(max_depth=25,max_leaf_nodes=500)"
      ],
      "metadata": {
        "id": "8_7EZkHQ52kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "PNHFu8dG54wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `max_features`\n",
        "\n",
        "Instead of picking all features (columns) for every split, we can specify that only a fraction of features be chosen randomly to figure out a split.\n",
        "\n",
        "<img src=\"https://i.imgur.com/FXGWMDY.png\" width=\"720\">\n",
        "\n",
        "Notice that the default value `auto` causes only $\\sqrt{n}$ out of total features ( $n$ ) to be chosen randomly at each split. This is the reason each decision tree in the forest is different. While it may seem counterintuitive, choosing all features for every split of every tree will lead to identical trees, so the random forest will not generalize well. "
      ],
      "metadata": {
        "id": "Sw_Mkt3a5vAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(max_features='log2')"
      ],
      "metadata": {
        "id": "3YrmcS5s5q81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(max_features=6)"
      ],
      "metadata": {
        "id": "uKrhoEl36DkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "TQuUnR0y6FdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `min_samples_split` and `min_samples_leaf`\n",
        "\n",
        "By default, the decision tree classifier tries to split every node that has 2 or more. You can increase the values of these arguments to change this behavior and reduce overfitting, especially for very large datasets."
      ],
      "metadata": {
        "id": "mS4s2Fm65-QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(min_samples_split=3, min_samples_leaf=2)"
      ],
      "metadata": {
        "id": "N90bZVrP6S5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(min_samples_split=100, min_samples_leaf=60)"
      ],
      "metadata": {
        "id": "rwb818rD6W4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "gZlDhsku6UsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `min_impurity_decrease`\n",
        "\n",
        "This argument is used to control the threshold for splitting nodes. A node will be split if this split induces a decrease of the impurity (Gini index) greater than or equal to this value. It's default value is 0, and you can increase it to reduce overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "26xSZYiJ6agB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(min_impurity_decrease=1e-7)"
      ],
      "metadata": {
        "id": "fkt_QkT36aJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(min_impurity_decrease=1e-2)"
      ],
      "metadata": {
        "id": "pjIX2qjP6fEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "IqFxOZDk6g4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `bootstrap`, `max_samples` \n",
        "\n",
        "By default, a random forest doesn't use the entire dataset for training each decision tree. Instead it applies a technique called bootstrapping. For each tree, rows from the dataset are picked one by one randomly, with replacement i.e. some rows may not show up at all, while some rows may show up multiple times.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/W8UGaEA.png\" width=\"640\">\n",
        "\n",
        "Bootstrapping helps the random forest generalize better, because each decision tree only sees a fraction of th training set, and some rows randomly get higher weightage than others."
      ],
      "metadata": {
        "id": "P-iOQV1i6jfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(bootstrap=False)"
      ],
      "metadata": {
        "id": "biKWbOFl6kDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "C-QbJVCb6n5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When bootstrapping is enabled, you can also control the number or fraction of rows to be considered for each bootstrap using `max_samples`. This can further generalize the model.\n",
        "\n",
        "<img src=\"https://i.imgur.com/rsdrL1W.png\" width=\"640\">"
      ],
      "metadata": {
        "id": "JoYZUgyT6qnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(max_samples=0.9)"
      ],
      "metadata": {
        "id": "gY1CmTg66nZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "VICGP9Co6nUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `class_weight`"
      ],
      "metadata": {
        "id": "D02hurZl6zNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.classes_"
      ],
      "metadata": {
        "id": "rAIsAJYJ6z1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(class_weight='balanced')"
      ],
      "metadata": {
        "id": "Ud2T5z1W62qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_params(class_weight={'No': 1, 'Yes': 2})"
      ],
      "metadata": {
        "id": "ryaqVhvl67Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_accs"
      ],
      "metadata": {
        "id": "UFhcAdSM69X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've increased the accuracy from 84.5% with a single decision tree to 85.7% with a well-tuned random forest. Depending on the dataset and the kind of problem, you may or may not a see a significant improvement with hyperparameter tuning. \n",
        "\n",
        "This could be due to any of the following reasons:\n",
        "\n",
        "- We may not have found the right mix of hyperparameters to regularize (reduce overfitting) the model properly, and we should keep trying to improve the model.\n",
        "\n",
        "- We may have reached the limits of the modeling technique we're currently using (Random Forests), and we should try another modeling technique e.g. gradient boosting.\n",
        "\n",
        "- We may have reached the limits of what we can predict using the given amount of data, and we may need more data to improve the model.\n",
        "\n",
        "- We may have reached the limits of how well we can predict whether it will rain tomorrow using the given weather measurements, and we may need more features (columns) to further improve the model. In many cases, we can also generate new features using existing features (this is called feature engineering).\n",
        "\n",
        "- Whether it will rain tomorrow may be an inherently random or chaotic phenomenon which simply cannot be predicted beyond a certain accuracy any amount of data for any number of weather measurements with any modeling technique.  \n",
        "\n",
        "Remember that ultimately all models are wrong, but some are useful. If you can rely on the model we've created today to make a travel decision for tomorrow, then the model is useful, even though it may sometimes be wrong."
      ],
      "metadata": {
        "id": "Tu9EDoRN7Rpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's also compute the accuracy of our model on the test set."
      ],
      "metadata": {
        "id": "oO4vIBf17YKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test, test_targets)"
      ],
      "metadata": {
        "id": "Bv3qc2Rj7Gcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 6 -Training a Extremely Randomized Trees (ExtraTree)\n",
        "This algorithm is an effort to introduce more randomness to the bagging process. Tree\n",
        "splits are chosen completely at random from the range of values in the sample at each\n",
        "split, which allows us to reduce the variance of the model further – however, at the cost of\n",
        "a slight increase in bias"
      ],
      "metadata": {
        "id": "-lr7b5M528in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "kfold = cross_validation.StratifiedKFold(y=y_train, n_folds=5, random_state=2017)\n",
        "num_trees = 100\n",
        "\n",
        "model = ExtraTreesClassifier(n_estimators=num_trees)\n",
        "model.fit(train_inputs, train_targets)\n",
        "\n",
        "results = cross_validation.cross_val_score(model, train_inputs, train_targets, cv=kfold)\n",
        "print(\"\\nRandom Forest (Bagging) - Train : \", results.mean())\n",
        "\n",
        "print(\"Random Forest (Bagging) - Test : \", metrics.accuracy_score(model.predict(val_inputs), val_targets))"
      ],
      "metadata": {
        "id": "Nzcu2L643Euc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 7 -Training a Gradient Boosting"
      ],
      "metadata": {
        "id": "1fRUfqUv4NVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Using Gradient Boosting of 100 iterations\n",
        "kfold = cross_validation.StratifiedKFold(y=y_train, n_folds=5, random_state=2017)\n",
        "num_trees = 100\n",
        "\n",
        "model = GradientBoostingClassifier(n_estimators=num_trees, learning_rate=0.1, random_state=2017)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "results = cross_validation.cross_val_score(model, train_inputs, train_targets, cv=kfold)\n",
        "print \"\\nGradient Boosting - CV Train : %.2f\" % results.mean()\n",
        "\n",
        "print \"Gradient Boosting - Train : %.2f\" % metrics.accuracy_score(model.predict(train_inputs),results = cross_validation.cross_val_score(model, train_inputs, train_targets, cv=kfold)\n",
        ")\n",
        "print \"Gradient Boosting - Test : %.2f\" % metrics.accuracy_score(model.predict(X_test), y_test)"
      ],
      "metadata": {
        "id": "8veqr8X34I4h",
        "outputId": "09ee749f-22c5-49e6-cc6a-4c77453df941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-feb9a5d4a2c5>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print \"\\nGradient Boosting - CV Train : %.2f\" % results.mean()\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 8 -Training a Xgboost (eXtreme Gradient Boosting)"
      ],
      "metadata": {
        "id": "xNQq1NPo6lKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(n_estimators = num_rounds,objective= 'binary:logistic',seed=2017)\n",
        "# use early_stopping_rounds to stop the cv when there is no score imporovement\n",
        "model.fit(train_inputs, train_targets, early_stopping_rounds=20, eval_set=[(train_inputs, train_targets)], verbose=False)\n",
        "\n",
        "results = cross_validation.cross_val_score(model, train_inputs, train_targets, cv=kfold)\n",
        "print \"\\nxgBoost - CV Train : %.2f\" % results.mean()\n",
        "\n",
        "print \"xgBoost - Train : %.2f\" % metrics.accuracy_score(model.predict(train_inputs), train_targets)\n",
        "print \"xgBoost - Test : %.2f\" % metrics.accuracy_score(model.predict(val_inputs), val_targets)\n"
      ],
      "metadata": {
        "id": "6GTDqWaC6xzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 9 -Training a LightGBM"
      ],
      "metadata": {
        "id": "2ykshKk6y0pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the lightgbm model\n",
        "import lightgbm as lgb\n",
        "LGBM = lgb.LGBMClassifier()\n",
        "LGBM.fit(train_inputs, train_targets)"
      ],
      "metadata": {
        "id": "FdbEu9Bzy1fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_train(LGBM,train_inputs,train_targets)"
      ],
      "metadata": {
        "id": "CneQzCMkzMC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalaute_test(LGBM,val_inputs,val_targets)"
      ],
      "metadata": {
        "id": "zV790q3VzN7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter tuning using optuna"
      ],
      "metadata": {
        "id": "v4DsxfLL1FqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "3oD9RIY01M3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna  \n",
        "from sklearn.metrics import log_loss\n",
        "from optuna.integration import LightGBMPruningCallback\n",
        "\n",
        "def objective(trial, X, y):\n",
        "  \n",
        "    param_grid = {\n",
        "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
        "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
        "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
        "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
        "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n",
        "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "   \n",
        "        model = lgbm.LGBMClassifier(objective=\"binary\", **param_grid)\n",
        "\n",
        "        model.fit(train_inputs,train_targets,\n",
        "            eval_set=[(val_inputs, val_targets)],\n",
        "            eval_metric=\"binary_logloss\",\n",
        "            early_stopping_rounds=100,\n",
        "            callbacks=[LightGBMPruningCallback(trial, \"binary_logloss\")])\n",
        "        \n",
        "        preds = model.predict_proba(val_inputs)\n",
        "        Accuracy=  log_loss(val_targets, preds)\n",
        "\n",
        "    return Accuracy"
      ],
      "metadata": {
        "id": "PKmGKmOj1EwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
        "func = lambda trial: objective(trial, X, y)\n",
        "study.optimize(func, n_trials=20)"
      ],
      "metadata": {
        "id": "H82bv3-52eMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.bast_params "
      ],
      "metadata": {
        "id": "7I0kwIep2qLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model-10 Catboost"
      ],
      "metadata": {
        "id": "KT2Y7OZD374U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "   \n",
        "   param = {\n",
        "        \n",
        "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
        "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n",
        "        \"used_ram_limit\": \"3gb\",\n",
        "    }\n",
        "\n",
        "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
        "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "\n",
        "    Cat = CatBoostClassifier(**param)\n",
        "\n",
        "    Cat.fit(X_train, y_train, eval_set=[(X_test, y_test)], cat_features=categorical_features_indices,verbose=0, early_stopping_rounds=100)\n",
        "\n",
        "    preds = Cat.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, preds)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "7Y7ROJBK36jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50, timeout=600)"
      ],
      "metadata": {
        "id": "5Bz_5MnJ5j21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Predictions on New Inputs\n",
        "\n",
        "Let's define a helper function to make predictions on new inputs."
      ],
      "metadata": {
        "id": "9E00qXMM7c6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_input(model, single_input):\n",
        "    input_df = pd.DataFrame([single_input])\n",
        "    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n",
        "    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n",
        "    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols])\n",
        "    X_input = input_df[numeric_cols + encoded_cols]\n",
        "    pred = model.predict(X_input)[0]\n",
        "    prob = model.predict_proba(X_input)[0][list(model.classes_).index(pred)]\n",
        "    return pred, prob"
      ],
      "metadata": {
        "id": "kSgv1NaH7GUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_input = {'Date': '2021-06-19',\n",
        "             'Location': 'Launceston',\n",
        "             'MinTemp': 23.2,\n",
        "             'MaxTemp': 33.2,\n",
        "             'Rainfall': 10.2,\n",
        "             'Evaporation': 4.2,\n",
        "             'Sunshine': np.nan,\n",
        "             'WindGustDir': 'NNW',\n",
        "             'WindGustSpeed': 52.0,\n",
        "             'WindDir9am': 'NW',\n",
        "             'WindDir3pm': 'NNE',\n",
        "             'WindSpeed9am': 13.0,\n",
        "             'WindSpeed3pm': 20.0,\n",
        "             'Humidity9am': 89.0,\n",
        "             'Humidity3pm': 58.0,\n",
        "             'Pressure9am': 1004.8,\n",
        "             'Pressure3pm': 1001.5,\n",
        "             'Cloud9am': 8.0,\n",
        "             'Cloud3pm': 5.0,\n",
        "             'Temp9am': 25.7,\n",
        "             'Temp3pm': 33.0,\n",
        "             'RainToday': 'Yes'}"
      ],
      "metadata": {
        "id": "BeYY4Krf7GKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Trained Models\n",
        "\n",
        "We can save the parameters (weights and biases) of our trained model to disk, so that we needn't retrain the model from scratch each time we wish to use it. Along with the model, it's also important to save imputers, scalers, encoders and even column names. Anything that will be required while generating predictions using the model should be saved.\n",
        "\n",
        "We can use the `joblib` module to save and load Python objects on the disk. "
      ],
      "metadata": {
        "id": "X-10ZAzO7nQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "aussie_rain = {\n",
        "    'model': model,\n",
        "    'imputer': imputer,\n",
        "    'scaler': scaler,\n",
        "    'encoder': encoder,\n",
        "    'input_cols': input_cols,\n",
        "    'target_col': target_col,\n",
        "    'numeric_cols': numeric_cols,\n",
        "    'categorical_cols': categorical_cols,\n",
        "    'encoded_cols': encoded_cols\n",
        "}"
      ],
      "metadata": {
        "id": "5KWwe9TV7nzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(aussie_rain, 'aussie_rain.joblib')"
      ],
      "metadata": {
        "id": "n6txcGrx7sti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The object can be loaded back using `joblib.load`"
      ],
      "metadata": {
        "id": "PDx16a7-7xZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aussie_rain2 = joblib.load('aussie_rain.joblib')"
      ],
      "metadata": {
        "id": "sdCwtWXO71HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds2 = aussie_rain2['model'].predict(X_test)\n",
        "accuracy_score(test_targets, test_preds2)"
      ],
      "metadata": {
        "id": "4baJ4-jG7x1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and References\n",
        "\n",
        "The following topics were covered in this tutorial:\n",
        "\n",
        "- Downloading a real-world dataset\n",
        "- Preparing a dataset for training\n",
        "- Training and interpreting decision trees\n",
        "- Training and interpreting random forests\n",
        "- Overfitting, hyperparameter tuning & regularization\n",
        "- Making predictions on single inputs\n",
        "\n",
        "\n",
        "\n",
        "We also introduced the following terms:\n",
        "\n",
        "* Decision tree\n",
        "* Random forest\n",
        "* Overfitting\n",
        "* Hyperparameter\n",
        "* Hyperparameter tuning\n",
        "* Regularization\n",
        "* Ensembling\n",
        "* Generalization\n",
        "* Bootstrapping\n",
        "\n",
        "\n",
        "Check out the following resources to learn more: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/tree.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "- https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n",
        "- https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering\n",
        "- https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search"
      ],
      "metadata": {
        "id": "aMIT3DQ48A9e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KsGs5gse8BhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}